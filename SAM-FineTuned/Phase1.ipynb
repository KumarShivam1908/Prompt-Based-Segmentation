{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "469323b9",
   "metadata": {
    "papermill": {
     "duration": 0.00233,
     "end_time": "2025-10-27T19:10:23.892882",
     "exception": false,
     "start_time": "2025-10-27T19:10:23.890552",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Phase-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8339175",
   "metadata": {
    "papermill": {
     "duration": 0.001449,
     "end_time": "2025-10-27T19:10:23.896357",
     "exception": false,
     "start_time": "2025-10-27T19:10:23.894908",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "#### Data-Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24c48a79",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-27T19:10:23.900603Z",
     "iopub.status.busy": "2025-10-27T19:10:23.900375Z",
     "iopub.status.idle": "2025-10-27T19:10:43.079831Z",
     "shell.execute_reply": "2025-10-27T19:10:43.078979Z"
    },
    "papermill": {
     "duration": 19.182887,
     "end_time": "2025-10-27T19:10:43.080917",
     "exception": false,
     "start_time": "2025-10-27T19:10:23.898030",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting roboflow\r\n",
      "  Downloading roboflow-1.2.11-py3-none-any.whl.metadata (9.7 kB)\r\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from roboflow) (2025.8.3)\r\n",
      "Collecting idna==3.7 (from roboflow)\r\n",
      "  Downloading idna-3.7-py3-none-any.whl.metadata (9.9 kB)\r\n",
      "Requirement already satisfied: cycler in /usr/local/lib/python3.11/dist-packages (from roboflow) (0.12.1)\r\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.4.8)\r\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from roboflow) (3.7.2)\r\n",
      "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.26.4)\r\n",
      "Collecting opencv-python-headless==4.10.0.84 (from roboflow)\r\n",
      "  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\r\n",
      "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from roboflow) (11.3.0)\r\n",
      "Collecting pi-heif<2 (from roboflow)\r\n",
      "  Downloading pi_heif-1.1.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.5 kB)\r\n",
      "Collecting pillow-avif-plugin<2 (from roboflow)\r\n",
      "  Downloading pillow_avif_plugin-1.5.2-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\r\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.9.0.post0)\r\n",
      "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.1.1)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.32.5)\r\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.17.0)\r\n",
      "Requirement already satisfied: urllib3>=1.26.6 in /usr/local/lib/python3.11/dist-packages (from roboflow) (2.5.0)\r\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from roboflow) (4.67.1)\r\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from roboflow) (6.0.3)\r\n",
      "Requirement already satisfied: requests-toolbelt in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.0.0)\r\n",
      "Requirement already satisfied: filetype in /usr/local/lib/python3.11/dist-packages (from roboflow) (1.2.0)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->roboflow) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->roboflow) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->roboflow) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->roboflow) (2025.2.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->roboflow) (2022.2.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.18.5->roboflow) (2.4.1)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (1.3.2)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (4.59.0)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (25.0)\r\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->roboflow) (3.0.9)\r\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->roboflow) (3.4.3)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.18.5->roboflow) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.18.5->roboflow) (2022.2.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.18.5->roboflow) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.18.5->roboflow) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.18.5->roboflow) (2024.2.0)\r\n",
      "Downloading roboflow-1.2.11-py3-none-any.whl (89 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading idna-3.7-py3-none-any.whl (66 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.8/66.8 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading pi_heif-1.1.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading pillow_avif_plugin-1.5.2-cp311-cp311-manylinux_2_28_x86_64.whl (4.2 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m99.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: pillow-avif-plugin, pi-heif, idna, opencv-python-headless, roboflow\r\n",
      "  Attempting uninstall: idna\r\n",
      "    Found existing installation: idna 3.10\r\n",
      "    Uninstalling idna-3.10:\r\n",
      "      Successfully uninstalled idna-3.10\r\n",
      "  Attempting uninstall: opencv-python-headless\r\n",
      "    Found existing installation: opencv-python-headless 4.12.0.88\r\n",
      "    Uninstalling opencv-python-headless-4.12.0.88:\r\n",
      "      Successfully uninstalled opencv-python-headless-4.12.0.88\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\r\n",
      "datasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\r\n",
      "google-cloud-bigtable 2.32.0 requires google-api-core[grpc]<3.0.0,>=2.17.0, but you have google-api-core 1.34.1 which is incompatible.\r\n",
      "google-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\r\n",
      "google-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\r\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\r\n",
      "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\r\n",
      "google-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\r\n",
      "bigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\r\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\r\n",
      "gradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\r\n",
      "pydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.1 which is incompatible.\r\n",
      "pydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\r\n",
      "pandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\r\n",
      "google-cloud-storage 2.19.0 requires google-api-core<3.0.0dev,>=2.15.0, but you have google-api-core 1.34.1 which is incompatible.\r\n",
      "transformers 4.53.3 requires huggingface-hub<1.0,>=0.30.0, but you have huggingface-hub 1.0.0rc2 which is incompatible.\r\n",
      "jupyter-kernel-gateway 2.5.2 requires jupyter-client<8.0,>=5.2.0, but you have jupyter-client 8.6.3 which is incompatible.\r\n",
      "dataproc-spark-connect 0.8.3 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\r\n",
      "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.9.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed idna-3.7 opencv-python-headless-4.10.0.84 pi-heif-1.1.1 pillow-avif-plugin-1.5.2 roboflow-1.2.11\r\n",
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in cracks-1 to coco:: 100%|██████████| 297149/297149 [00:03<00:00, 88679.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Dataset Version Zip to cracks-1 in coco:: 100%|██████████| 5377/5377 [00:01<00:00, 5341.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in Drywall-Join-Detect-1 to coco:: 100%|██████████| 28331/28331 [00:00<00:00, 53732.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Dataset Version Zip to Drywall-Join-Detect-1 in coco:: 100%|██████████| 1028/1028 [00:00<00:00, 7649.37it/s]\n"
     ]
    }
   ],
   "source": [
    "# Install Roboflow\n",
    "!pip install roboflow\n",
    "\n",
    "# Import and initialize Roboflow\n",
    "from roboflow import Roboflow\n",
    "\n",
    "rf = Roboflow(api_key=\"qxxPbuRLdmGnVgKsd0WL\")\n",
    "\n",
    "#  \"cracks\" dataset (version 1) in COCO format\n",
    "cracks_project = rf.workspace(\"machinelearning-u4gmu\").project(\"cracks-3ii36-jivvr\")\n",
    "cracks_dataset = cracks_project.version(1).download(\"coco\")\n",
    "\n",
    "# \"drywall join detect\" dataset (version 1) in COCO format\n",
    "drywall_project = rf.workspace(\"machinelearning-u4gmu\").project(\"drywall-join-detect-ioimg\")\n",
    "drywall_dataset = drywall_project.version(1).download(\"coco\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b118714",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T19:10:43.093655Z",
     "iopub.status.busy": "2025-10-27T19:10:43.093162Z",
     "iopub.status.idle": "2025-10-27T19:10:43.097691Z",
     "shell.execute_reply": "2025-10-27T19:10:43.096979Z"
    },
    "papermill": {
     "duration": 0.011683,
     "end_time": "2025-10-27T19:10:43.098793",
     "exception": false,
     "start_time": "2025-10-27T19:10:43.087110",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cracks dataset location: /kaggle/working/cracks-1\n",
      "Drywall dataset location: /kaggle/working/Drywall-Join-Detect-1\n"
     ]
    }
   ],
   "source": [
    "# Store the paths for use in later cells\n",
    "CRACKS_PATH = cracks_dataset.location\n",
    "DRYWALL_PATH = drywall_dataset.location\n",
    "\n",
    "# Print the actual paths where datasets were downloaded\n",
    "print(f\"\\nCracks dataset location: {CRACKS_PATH}\")\n",
    "print(f\"Drywall dataset location: {DRYWALL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e64c373",
   "metadata": {
    "papermill": {
     "duration": 0.005344,
     "end_time": "2025-10-27T19:10:43.109666",
     "exception": false,
     "start_time": "2025-10-27T19:10:43.104322",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Mask Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ff27b94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T19:10:43.122286Z",
     "iopub.status.busy": "2025-10-27T19:10:43.121816Z",
     "iopub.status.idle": "2025-10-27T19:10:56.837073Z",
     "shell.execute_reply": "2025-10-27T19:10:56.835924Z"
    },
    "papermill": {
     "duration": 13.722605,
     "end_time": "2025-10-27T19:10:56.838420",
     "exception": false,
     "start_time": "2025-10-27T19:10:43.115815",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/facebookresearch/segment-anything.git\r\n",
      "  Cloning https://github.com/facebookresearch/segment-anything.git to /tmp/pip-req-build-zgvyf3pr\r\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything.git /tmp/pip-req-build-zgvyf3pr\r\n",
      "  Resolved https://github.com/facebookresearch/segment-anything.git to commit dca509fe793f601edb92606367a655c15ac00fdf\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Checkpoint 'sam_vit_h_4b8939.pth' not found. Downloading...\n",
      "--2025-10-27 19:10:49--  https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\r\n",
      "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.225.143.99, 13.225.143.109, 13.225.143.54, ...\r\n",
      "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.225.143.99|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 2564550879 (2.4G) [binary/octet-stream]\r\n",
      "Saving to: ‘sam_vit_h_4b8939.pth’\r\n",
      "\r\n",
      "sam_vit_h_4b8939.pt 100%[===================>]   2.39G   343MB/s    in 7.3s    \r\n",
      "\r\n",
      "2025-10-27 19:10:56 (335 MB/s) - ‘sam_vit_h_4b8939.pth’ saved [2564550879/2564550879]\r\n",
      "\r\n",
      "Downloaded 'sam_vit_h_4b8939.pth' successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Install SAM\n",
    "!pip install git+https://github.com/facebookresearch/segment-anything.git\n",
    "\n",
    "checkpoint_path = \"sam_vit_h_4b8939.pth\"\n",
    "\n",
    "if not os.path.exists(checkpoint_path):\n",
    "    print(f\"Checkpoint '{checkpoint_path}' not found. Downloading...\")\n",
    "    \n",
    "    # Download the official SAM checkpoint\n",
    "    !wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
    "    \n",
    "    # Check if the download was successful\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        print(f\"Downloaded '{checkpoint_path}' successfully.\")\n",
    "    else:\n",
    "        print(f\"--- DOWNLOAD FAILED ---\")\n",
    "        print(f\"The 'wget' command failed. Please check the notebook's internet connection.\")\n",
    "else:\n",
    "    print(f\"Checkpoint '{checkpoint_path}' already exists. Skipping download.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ead1cef1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T19:10:56.855663Z",
     "iopub.status.busy": "2025-10-27T19:10:56.855243Z",
     "iopub.status.idle": "2025-10-27T23:26:27.211240Z",
     "shell.execute_reply": "2025-10-27T23:26:27.210371Z"
    },
    "papermill": {
     "duration": 15330.365779,
     "end_time": "2025-10-27T23:26:27.212319",
     "exception": false,
     "start_time": "2025-10-27T19:10:56.846540",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using dataset paths:\n",
      "  - Cracks: /kaggle/working/cracks-1\n",
      "  - Drywall: /kaggle/working/Drywall-Join-Detect-1\n",
      "\n",
      "Using device: cuda\n",
      "Loading SAM model...\n",
      "Model loaded successfully.\n",
      "\n",
      "\n",
      "============================================================\n",
      "Processing dataset: cracks-1\n",
      "============================================================\n",
      "\n",
      "  Processing train split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  cracks-1/train: 100%|██████████| 5164/5164 [3:26:06<00:00,  2.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Completed train split (5164 images)\n",
      "\n",
      "  Processing valid split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  cracks-1/valid: 100%|██████████| 201/201 [08:01<00:00,  2.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Completed valid split (201 images)\n",
      "\n",
      "  Processing test split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  cracks-1/test: 100%|██████████| 4/4 [00:09<00:00,  2.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Completed test split (4 images)\n",
      "\n",
      "============================================================\n",
      "Processing dataset: Drywall-Join-Detect-1\n",
      "============================================================\n",
      "\n",
      "  Processing train split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Drywall-Join-Detect-1/train: 100%|██████████| 820/820 [32:47<00:00,  2.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Completed train split (820 images)\n",
      "\n",
      "  Processing valid split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Drywall-Join-Detect-1/valid: 100%|██████████| 202/202 [08:04<00:00,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✓ Completed valid split (202 images)\n",
      "  Skipping test (folder not found)\n",
      "\n",
      "============================================================\n",
      "✓ Mask generation complete!\n",
      "  All masks saved to 'processed_masks'\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import SAM\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "\n",
    "def load_coco_annotations(dataset_dir):\n",
    "    \"\"\"\n",
    "    Loads COCO annotations from a Roboflow export.\n",
    "    Roboflow typically names the file '_annotations.coco.json'.\n",
    "    \"\"\"\n",
    "    ann_path = os.path.join(dataset_dir, '_annotations.coco.json')\n",
    "    \n",
    "    if not os.path.exists(ann_path):\n",
    "        raise FileNotFoundError(f\"Could not find '_annotations.coco.json' in '{dataset_dir}'.\")\n",
    "\n",
    "    with open(ann_path, \"r\") as f:\n",
    "        coco = json.load(f)\n",
    "\n",
    "    images = {img[\"id\"]: img[\"file_name\"] for img in coco[\"images\"]}\n",
    "    annos = coco[\"annotations\"]\n",
    "    bbox_map = {}\n",
    "    for ann in annos:\n",
    "        img_id = ann[\"image_id\"]\n",
    "        bbox_coco = ann[\"bbox\"]  # COCO format: [x_min, y_min, width, height]\n",
    "        x_min, y_min, w, h = bbox_coco\n",
    "        bbox_xyxy = [x_min, y_min, x_min + w, y_min + h]\n",
    "        bbox_map.setdefault(img_id, []).append(bbox_xyxy)\n",
    "        \n",
    "    return images, bbox_map\n",
    "\n",
    "def generate_and_save_masks(dataset_dirs, predictor, output_dir):\n",
    "    \"\"\"\n",
    "    Iterates through specified dataset directories (train/valid/test splits),\n",
    "    generates a single combined mask for each image, and saves them to a unified output directory.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for dataset_root in dataset_dirs:\n",
    "        dataset_name = os.path.basename(os.path.normpath(dataset_root))\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Processing dataset: {dataset_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Process train, valid, and test splits\n",
    "        for split in ['train', 'valid', 'test']:\n",
    "            split_dir = os.path.join(dataset_root, split)\n",
    "            \n",
    "            if not os.path.exists(split_dir):\n",
    "                print(f\"  Skipping {split} (folder not found)\")\n",
    "                continue\n",
    "                \n",
    "            print(f\"\\n  Processing {split} split...\")\n",
    "            \n",
    "            try:\n",
    "                images, bbox_map = load_coco_annotations(split_dir)\n",
    "            except FileNotFoundError as e:\n",
    "                print(f\"  Error: {e}\")\n",
    "                continue\n",
    "            \n",
    "            for img_id, img_name in tqdm(images.items(), desc=f\"  {dataset_name}/{split}\"):\n",
    "                img_path = os.path.join(split_dir, img_name)\n",
    "                if not os.path.exists(img_path):\n",
    "                    print(f\"    Warning: Image file not found at {img_path}. Skipping.\")\n",
    "                    continue\n",
    "\n",
    "                bboxes = bbox_map.get(img_id)\n",
    "                if not bboxes:\n",
    "                    continue\n",
    "\n",
    "                image_bgr = cv2.imread(img_path)\n",
    "                if image_bgr is None:\n",
    "                    print(f\"    Warning: Could not read image {img_path}. Skipping.\")\n",
    "                    continue\n",
    "                \n",
    "                image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "                predictor.set_image(image_rgb)\n",
    "                \n",
    "                h, w = image_rgb.shape[:2]\n",
    "                combined_mask = np.zeros((h, w), dtype=bool)\n",
    "\n",
    "                for bbox in bboxes:\n",
    "                    box_np = np.array(bbox)\n",
    "                    masks, scores, logits = predictor.predict(\n",
    "                        box=box_np,\n",
    "                        multimask_output=False\n",
    "                    )\n",
    "                    combined_mask = np.logical_or(combined_mask, masks[0])\n",
    "                \n",
    "                # Save mask with dataset and split info in filename\n",
    "                base_name = os.path.splitext(img_name)[0]\n",
    "                out_name = f\"{dataset_name}_{split}_{base_name}.png\"\n",
    "                out_path = os.path.join(output_dir, out_name)\n",
    "                cv2.imwrite(out_path, combined_mask.astype(np.uint8) * 255)\n",
    "            \n",
    "            print(f\"  ✓ Completed {split} split ({len(images)} images)\")\n",
    "\n",
    "def main():\n",
    "    # Use the paths from the data loading cell\n",
    "    dataset_dirs = [\n",
    "        CRACKS_PATH,      # Path from Roboflow download\n",
    "        DRYWALL_PATH       # Path from Roboflow download\n",
    "    ]\n",
    "    checkpoint_path = \"sam_vit_h_4b8939.pth\"\n",
    "    output_dir = \"processed_masks\"\n",
    "    \n",
    "    print(f\"Using dataset paths:\")\n",
    "    print(f\"  - Cracks: {CRACKS_PATH}\")\n",
    "    print(f\"  - Drywall: {DRYWALL_PATH}\\n\")\n",
    "    \n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        print(f\"Checkpoint file not found at '{checkpoint_path}'\")\n",
    "        print(\"Please run the previous cell to download the model checkpoint.\")\n",
    "        return\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    print(\"Loading SAM model...\")\n",
    "    sam = sam_model_registry[\"vit_h\"](checkpoint=checkpoint_path)\n",
    "    sam.to(device=device)\n",
    "    predictor = SamPredictor(sam)\n",
    "    print(\"Model loaded successfully.\\n\")\n",
    "\n",
    "    generate_and_save_masks(dataset_dirs, predictor, output_dir)\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"✓ Mask generation complete!\")\n",
    "    print(f\"  All masks saved to '{output_dir}'\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b17c380",
   "metadata": {
    "papermill": {
     "duration": 0.252799,
     "end_time": "2025-10-27T23:26:27.721848",
     "exception": false,
     "start_time": "2025-10-27T23:26:27.469049",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 15369.83684,
   "end_time": "2025-10-27T23:26:29.700280",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-27T19:10:19.863440",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
