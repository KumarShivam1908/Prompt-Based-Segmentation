{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!ls /kaggle/input/prompt-based-segmentaion/final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-29T11:36:21.268171Z",
     "iopub.status.busy": "2025-10-29T11:36:21.267769Z",
     "iopub.status.idle": "2025-10-29T13:16:32.914989Z",
     "shell.execute_reply": "2025-10-29T13:16:32.913950Z",
     "shell.execute_reply.started": "2025-10-29T11:36:21.268143Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-29 11:36:26.875251: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1761737786.906731     988 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1761737786.914648     988 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Models will be saved to clipseg_models\n",
      "======================================================================\n",
      "CLIPSEG FINE-TUNING\n",
      "======================================================================\n",
      "\n",
      "Loading model: CIDAS/clipseg-rd64-refined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Froze CLIP text and vision backbones.\n",
      "✓ Trainable params: 1,127,009 (0.75%)\n",
      "✓ Frozen params: 149,620,737\n",
      "✓ Loaded 8590 samples from /kaggle/input/prompt-based-segmentaion/final_dataset/train.csv\n",
      "✓ Loaded 1074 samples from /kaggle/input/prompt-based-segmentaion/final_dataset/valid.csv\n",
      "\n",
      "✓ Train batches: 1074\n",
      "✓ Valid batches: 135\n",
      "\n",
      "======================================================================\n",
      "STARTING TRAINING\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]: 100%|██████████| 1074/1074 [02:30<00:00,  7.13it/s, loss=0.2012, iou=0.2351, dice=0.3806]\n",
      "Validating: 100%|██████████| 135/135 [00:48<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1\n",
      "Train: Loss=0.8133, IoU=0.1645, Dice=0.2748\n",
      "Valid: Loss=0.7321, IoU=0.2251, Dice=0.3595\n",
      "✅ Best model saved! IoU: 0.2251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 [Train]: 100%|██████████| 1074/1074 [02:30<00:00,  7.13it/s, loss=0.1672, iou=0.1953, dice=0.3268]\n",
      "Validating: 100%|██████████| 135/135 [00:47<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2\n",
      "Train: Loss=0.7306, IoU=0.2255, Dice=0.3576\n",
      "Valid: Loss=0.6867, IoU=0.2668, Dice=0.4112\n",
      "✅ Best model saved! IoU: 0.2668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 [Train]: 100%|██████████| 1074/1074 [02:31<00:00,  7.11it/s, loss=0.2035, iou=0.1183, dice=0.2116]\n",
      "Validating: 100%|██████████| 135/135 [00:47<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3\n",
      "Train: Loss=0.7090, IoU=0.2480, Dice=0.3851\n",
      "Valid: Loss=0.6741, IoU=0.2733, Dice=0.4193\n",
      "✅ Best model saved! IoU: 0.2733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 [Train]: 100%|██████████| 1074/1074 [02:31<00:00,  7.11it/s, loss=0.1987, iou=0.1095, dice=0.1975]\n",
      "Validating: 100%|██████████| 135/135 [00:47<00:00,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4\n",
      "Train: Loss=0.7002, IoU=0.2577, Dice=0.3984\n",
      "Valid: Loss=0.6698, IoU=0.2806, Dice=0.4273\n",
      "✅ Best model saved! IoU: 0.2806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5 [Train]: 100%|██████████| 1074/1074 [02:31<00:00,  7.08it/s, loss=0.2221, iou=0.0625, dice=0.1177]\n",
      "Validating: 100%|██████████| 135/135 [00:47<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5\n",
      "Train: Loss=0.6940, IoU=0.2603, Dice=0.4006\n",
      "Valid: Loss=0.6667, IoU=0.2848, Dice=0.4320\n",
      "✅ Best model saved! IoU: 0.2848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6 [Train]: 100%|██████████| 1074/1074 [02:31<00:00,  7.09it/s, loss=0.1750, iou=0.2512, dice=0.4015]\n",
      "Validating: 100%|██████████| 135/135 [00:47<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6\n",
      "Train: Loss=0.6934, IoU=0.2610, Dice=0.4015\n",
      "Valid: Loss=0.6565, IoU=0.2854, Dice=0.4328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7 [Train]: 100%|██████████| 1074/1074 [02:31<00:00,  7.11it/s, loss=0.1370, iou=0.3256, dice=0.4913]\n",
      "Validating: 100%|██████████| 135/135 [00:47<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7\n",
      "Train: Loss=0.6838, IoU=0.2699, Dice=0.4130\n",
      "Valid: Loss=0.6465, IoU=0.2988, Dice=0.4489\n",
      "✅ Best model saved! IoU: 0.2988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8 [Train]: 100%|██████████| 1074/1074 [02:31<00:00,  7.10it/s, loss=0.1691, iou=0.2670, dice=0.4214]\n",
      "Validating: 100%|██████████| 135/135 [00:47<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8\n",
      "Train: Loss=0.6768, IoU=0.2745, Dice=0.4181\n",
      "Valid: Loss=0.6384, IoU=0.3147, Dice=0.4657\n",
      "✅ Best model saved! IoU: 0.3147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9 [Train]: 100%|██████████| 1074/1074 [02:31<00:00,  7.10it/s, loss=0.1441, iou=0.2502, dice=0.4003]\n",
      "Validating: 100%|██████████| 135/135 [00:47<00:00,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9\n",
      "Train: Loss=0.6706, IoU=0.2808, Dice=0.4258\n",
      "Valid: Loss=0.6325, IoU=0.3198, Dice=0.4720\n",
      "✅ Best model saved! IoU: 0.3198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10 [Train]: 100%|██████████| 1074/1074 [02:30<00:00,  7.12it/s, loss=0.1794, iou=0.1365, dice=0.2402]\n",
      "Validating: 100%|██████████| 135/135 [00:47<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10\n",
      "Train: Loss=0.6627, IoU=0.2820, Dice=0.4278\n",
      "Valid: Loss=0.6269, IoU=0.3277, Dice=0.4803\n",
      "✅ Best model saved! IoU: 0.3277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11 [Train]: 100%|██████████| 1074/1074 [02:31<00:00,  7.11it/s, loss=0.1359, iou=0.3877, dice=0.5588]\n",
      "Validating: 100%|██████████| 135/135 [00:47<00:00,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11\n",
      "Train: Loss=0.6603, IoU=0.2871, Dice=0.4329\n",
      "Valid: Loss=0.6264, IoU=0.3244, Dice=0.4771\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12 [Train]: 100%|██████████| 1074/1074 [02:31<00:00,  7.11it/s, loss=0.1823, iou=0.2859, dice=0.4447]\n",
      "Validating: 100%|██████████| 135/135 [00:47<00:00,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12\n",
      "Train: Loss=0.6562, IoU=0.2887, Dice=0.4339\n",
      "Valid: Loss=0.6222, IoU=0.3307, Dice=0.4841\n",
      "✅ Best model saved! IoU: 0.3307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13 [Train]: 100%|██████████| 1074/1074 [02:30<00:00,  7.12it/s, loss=0.1884, iou=0.2346, dice=0.3800]\n",
      "Validating: 100%|██████████| 135/135 [00:47<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13\n",
      "Train: Loss=0.6561, IoU=0.2904, Dice=0.4367\n",
      "Valid: Loss=0.6224, IoU=0.3284, Dice=0.4819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14 [Train]: 100%|██████████| 1074/1074 [02:30<00:00,  7.12it/s, loss=0.1653, iou=0.1918, dice=0.3219]\n",
      "Validating: 100%|██████████| 135/135 [00:47<00:00,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14\n",
      "Train: Loss=0.6548, IoU=0.2910, Dice=0.4382\n",
      "Valid: Loss=0.6218, IoU=0.3313, Dice=0.4847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15 [Train]: 100%|██████████| 1074/1074 [02:30<00:00,  7.12it/s, loss=0.1693, iou=0.2341, dice=0.3793]\n",
      "Validating: 100%|██████████| 135/135 [00:47<00:00,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15\n",
      "Train: Loss=0.6543, IoU=0.2897, Dice=0.4366\n",
      "Valid: Loss=0.6216, IoU=0.3310, Dice=0.4844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16 [Train]: 100%|██████████| 1074/1074 [02:31<00:00,  7.11it/s, loss=0.1625, iou=0.6047, dice=0.7537]\n",
      "Validating: 100%|██████████| 135/135 [00:47<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16\n",
      "Train: Loss=0.6523, IoU=0.2945, Dice=0.4416\n",
      "Valid: Loss=0.6201, IoU=0.3324, Dice=0.4858\n",
      "✅ Best model saved! IoU: 0.3324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17 [Train]: 100%|██████████| 1074/1074 [02:30<00:00,  7.12it/s, loss=0.1557, iou=0.2111, dice=0.3486]\n",
      "Validating: 100%|██████████| 135/135 [00:47<00:00,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17\n",
      "Train: Loss=0.6524, IoU=0.2939, Dice=0.4410\n",
      "Valid: Loss=0.6139, IoU=0.3384, Dice=0.4919\n",
      "✅ Best model saved! IoU: 0.3384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18 [Train]: 100%|██████████| 1074/1074 [02:30<00:00,  7.13it/s, loss=0.1815, iou=0.1958, dice=0.3275]\n",
      "Validating: 100%|██████████| 135/135 [00:47<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18\n",
      "Train: Loss=0.6483, IoU=0.3002, Dice=0.4484\n",
      "Valid: Loss=0.6091, IoU=0.3433, Dice=0.4981\n",
      "✅ Best model saved! IoU: 0.3433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19 [Train]: 100%|██████████| 1074/1074 [02:30<00:00,  7.12it/s, loss=0.1523, iou=0.4625, dice=0.6325]\n",
      "Validating: 100%|██████████| 135/135 [00:47<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19\n",
      "Train: Loss=0.6418, IoU=0.3025, Dice=0.4509\n",
      "Valid: Loss=0.6111, IoU=0.3470, Dice=0.5011\n",
      "✅ Best model saved! IoU: 0.3470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20 [Train]: 100%|██████████| 1074/1074 [02:30<00:00,  7.12it/s, loss=0.1230, iou=0.2802, dice=0.4377]\n",
      "Validating: 100%|██████████| 135/135 [00:47<00:00,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20\n",
      "Train: Loss=0.6392, IoU=0.3096, Dice=0.4591\n",
      "Valid: Loss=0.6031, IoU=0.3531, Dice=0.5086\n",
      "✅ Best model saved! IoU: 0.3531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21 [Train]: 100%|██████████| 1074/1074 [02:30<00:00,  7.13it/s, loss=0.1873, iou=0.1246, dice=0.2215]\n",
      "Validating: 100%|██████████| 135/135 [00:47<00:00,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21\n",
      "Train: Loss=0.6356, IoU=0.3095, Dice=0.4592\n",
      "Valid: Loss=0.6004, IoU=0.3558, Dice=0.5111\n",
      "✅ Best model saved! IoU: 0.3558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22 [Train]: 100%|██████████| 1074/1074 [02:31<00:00,  7.11it/s, loss=0.2065, iou=0.1517, dice=0.2635]\n",
      "Validating: 100%|██████████| 135/135 [00:47<00:00,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22\n",
      "Train: Loss=0.6350, IoU=0.3099, Dice=0.4598\n",
      "Valid: Loss=0.6010, IoU=0.3522, Dice=0.5081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23 [Train]: 100%|██████████| 1074/1074 [02:30<00:00,  7.11it/s, loss=0.1775, iou=0.1841, dice=0.3109]\n",
      "Validating: 100%|██████████| 135/135 [00:47<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23\n",
      "Train: Loss=0.6315, IoU=0.3164, Dice=0.4669\n",
      "Valid: Loss=0.5961, IoU=0.3627, Dice=0.5183\n",
      "✅ Best model saved! IoU: 0.3627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24 [Train]: 100%|██████████| 1074/1074 [02:30<00:00,  7.12it/s, loss=0.1620, iou=0.2995, dice=0.4609]\n",
      "Validating: 100%|██████████| 135/135 [00:47<00:00,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24\n",
      "Train: Loss=0.6258, IoU=0.3185, Dice=0.4692\n",
      "Valid: Loss=0.5936, IoU=0.3632, Dice=0.5197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25 [Train]: 100%|██████████| 1074/1074 [02:30<00:00,  7.12it/s, loss=0.1137, iou=0.3848, dice=0.5557]\n",
      "Validating: 100%|██████████| 135/135 [00:47<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 25\n",
      "Train: Loss=0.6252, IoU=0.3207, Dice=0.4720\n",
      "Valid: Loss=0.5963, IoU=0.3615, Dice=0.5175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26 [Train]: 100%|██████████| 1074/1074 [02:30<00:00,  7.13it/s, loss=0.1476, iou=0.3423, dice=0.5100]\n",
      "Validating: 100%|██████████| 135/135 [00:47<00:00,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 26\n",
      "Train: Loss=0.6274, IoU=0.3179, Dice=0.4684\n",
      "Valid: Loss=0.5923, IoU=0.3652, Dice=0.5217\n",
      "✅ Best model saved! IoU: 0.3652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27 [Train]: 100%|██████████| 1074/1074 [02:31<00:00,  7.11it/s, loss=0.1173, iou=0.5555, dice=0.7143]\n",
      "Validating: 100%|██████████| 135/135 [00:47<00:00,  2.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27\n",
      "Train: Loss=0.6231, IoU=0.3169, Dice=0.4677\n",
      "Valid: Loss=0.5905, IoU=0.3672, Dice=0.5234\n",
      "✅ Best model saved! IoU: 0.3672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28 [Train]: 100%|██████████| 1074/1074 [02:31<00:00,  7.11it/s, loss=0.1579, iou=0.2207, dice=0.3616]\n",
      "Validating: 100%|██████████| 135/135 [00:47<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28\n",
      "Train: Loss=0.6236, IoU=0.3220, Dice=0.4729\n",
      "Valid: Loss=0.5901, IoU=0.3643, Dice=0.5211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29 [Train]: 100%|██████████| 1074/1074 [02:31<00:00,  7.11it/s, loss=0.1441, iou=0.2540, dice=0.4051]\n",
      "Validating: 100%|██████████| 135/135 [00:47<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 29\n",
      "Train: Loss=0.6233, IoU=0.3190, Dice=0.4701\n",
      "Valid: Loss=0.5887, IoU=0.3665, Dice=0.5232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30 [Train]: 100%|██████████| 1074/1074 [02:30<00:00,  7.11it/s, loss=0.2031, iou=0.1443, dice=0.2522]\n",
      "Validating: 100%|██████████| 135/135 [00:47<00:00,  2.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 30\n",
      "Train: Loss=0.6223, IoU=0.3242, Dice=0.4750\n",
      "Valid: Loss=0.5878, IoU=0.3684, Dice=0.5254\n",
      "✅ Best model saved! IoU: 0.3684\n",
      "\n",
      "======================================================================\n",
      "TRAINING COMPLETE!\n",
      "======================================================================\n",
      "Best validation IoU: 0.3684\n",
      "Model saved to: clipseg_models/best_model.pth\n"
     ]
    }
   ],
   "source": [
    "\"\"\"ClipSeg fine-tuning script — updated to handle HF output shape mismatch and logits attribute name.\n",
    "\n",
    "Key fixes:\n",
    "- Use CLIPSegForImageSegmentation.from_pretrained(...) instead of AutoModelForImageSegmentation\n",
    "- Read logits from outputs.logits or outputs.conditional_logits (defensive)\n",
    "- Coerce logits to 4D (N, C, H, W) before F.interpolate\n",
    "- Defensive runtime checks and a single debug print (commented by default)\n",
    "\n",
    "Run this in the same environment as your original script. \"\"\"\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import ast\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Transformers import\n",
    "try:\n",
    "    from transformers import AutoProcessor\n",
    "    from transformers import CLIPSegForImageSegmentation\n",
    "except ImportError:\n",
    "    print(\"transformers not found. Please install it: !pip install -q transformers\")\n",
    "    raise\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "class Config:\n",
    "    # Paths\n",
    "    DATASET_DIR = \"/kaggle/input/prompt-based-segmentaion/final_dataset\"\n",
    "    SAVE_DIR = \"clipseg_models\"\n",
    "\n",
    "    # Model\n",
    "    MODEL_NAME = \"CIDAS/clipseg-rd64-refined\"\n",
    "    IMAGE_SIZE = 352\n",
    "\n",
    "    # Training\n",
    "    BATCH_SIZE = 8\n",
    "    NUM_EPOCHS = 30\n",
    "    LEARNING_RATE = 5e-5\n",
    "    WEIGHT_DECAY = 1e-2\n",
    "    NUM_WORKERS = 2\n",
    "\n",
    "    # Early stopping\n",
    "    PATIENCE = 5\n",
    "    MIN_DELTA = 0.001\n",
    "\n",
    "    # Mixed precision\n",
    "    USE_AMP = True\n",
    "    GRADIENT_ACCUMULATION_STEPS = 4\n",
    "    PYTORCH_CUDA_ALLOC_CONF = \"expandable_segments:True\"\n",
    "    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "    # Device\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# ============================================================================\n",
    "# SETUP: Create save directory\n",
    "# ============================================================================\n",
    "os.makedirs(config.SAVE_DIR, exist_ok=True)\n",
    "print(f\"✓ Models will be saved to {config.SAVE_DIR}\")\n",
    "\n",
    "# ============================================================================\n",
    "# Model Freezing Utility\n",
    "# ============================================================================\n",
    "\n",
    "def freeze_clip_backbone(model):\n",
    "    \"\"\"Freeze all parameters in the CLIP backbone\"\"\"\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"clip\" in name:\n",
    "            param.requires_grad = False\n",
    "    print(\"✓ Froze CLIP text and vision backbones.\")\n",
    "\n",
    "# ============================================================================\n",
    "# Dataset\n",
    "# ============================================================================\n",
    "class PromptSegDataset(Dataset):\n",
    "    \"\"\"Dataset for prompt-based segmentation (Image, Text, Mask)\"\"\"\n",
    "\n",
    "    def __init__(self, csv_path, base_dir, transform=None):\n",
    "        try:\n",
    "            self.data = pd.read_csv(csv_path, dtype=str)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading CSV: {e}\")\n",
    "            self.data = pd.read_csv(csv_path, dtype=str, encoding='utf-8')\n",
    "\n",
    "        self.base_dir = base_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        print(f\"✓ Loaded {len(self.data)} samples from {csv_path}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "\n",
    "        # Load image and mask\n",
    "        img_path = os.path.join(self.base_dir, row['image_path'])\n",
    "        mask_path = os.path.join(self.base_dir, row['mask_path'])\n",
    "\n",
    "        image = cv2.imread(img_path)\n",
    "        if image is None:\n",
    "            raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if mask is None:\n",
    "            raise FileNotFoundError(f\"Mask not found: {mask_path}\")\n",
    "        mask = (mask > 0).astype(np.uint8)\n",
    "\n",
    "        # Get text prompt\n",
    "        try:\n",
    "            prompts = ast.literal_eval(row['prompts'])\n",
    "            text_prompt = prompts[0] if prompts else \"object\"\n",
    "        except Exception:\n",
    "            text_prompt = \"object\"\n",
    "\n",
    "        # Apply augmentation\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "        else:\n",
    "            # Ensure returned types are tensors\n",
    "            image = ToTensorV2()(image=image)['image']\n",
    "            mask = torch.from_numpy(mask).unsqueeze(0).float()\n",
    "\n",
    "        return {\n",
    "            'image': image,\n",
    "            'mask': mask.float(),  # Ensure mask is float for loss\n",
    "            'prompt': text_prompt,\n",
    "            'class_name': row.get('class_name', 'object')\n",
    "        }\n",
    "\n",
    "# ============================================================================\n",
    "# Data Augmentation\n",
    "# ============================================================================\n",
    "\n",
    "def get_transforms(is_train=True):\n",
    "    \"\"\"Get augmentation pipeline for ClipSeg\"\"\"\n",
    "    CLIP_MEAN = [0.48145466, 0.4578275, 0.40821073]\n",
    "    CLIP_STD = [0.26862954, 0.26130258, 0.27577711]\n",
    "\n",
    "    if is_train:\n",
    "        return A.Compose([\n",
    "            A.Resize(height=config.IMAGE_SIZE, width=config.IMAGE_SIZE),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.3),\n",
    "            A.RandomBrightnessContrast(p=0.4),\n",
    "            A.GaussNoise(p=0.2),\n",
    "            A.GaussianBlur(blur_limit=3, p=0.2),\n",
    "            A.Affine(translate_percent=0.1, scale=(0.9, 1.1), rotate=(-15, 15), p=0.5),\n",
    "            A.Normalize(mean=CLIP_MEAN, std=CLIP_STD),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "    else:\n",
    "        return A.Compose([\n",
    "            A.Resize(height=config.IMAGE_SIZE, width=config.IMAGE_SIZE),\n",
    "            A.Normalize(mean=CLIP_MEAN, std=CLIP_STD),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "\n",
    "# ============================================================================\n",
    "# Custom Collate Function\n",
    "# ============================================================================\n",
    "class ClipSegCollator:\n",
    "    \"\"\"Custom collate function to handle text tokenization.\"\"\"\n",
    "    def __init__(self, processor):\n",
    "        self.processor = processor\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        pixel_values = torch.stack([item['image'] for item in batch])\n",
    "        masks = torch.stack([item['mask'] for item in batch])\n",
    "\n",
    "        texts = [item['prompt'] for item in batch]\n",
    "\n",
    "        inputs = self.processor(\n",
    "            text=texts,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'pixel_values': pixel_values,\n",
    "            'input_ids': inputs.input_ids,\n",
    "            'attention_mask': inputs.attention_mask,\n",
    "            'masks': masks,\n",
    "            'class_names': [item['class_name'] for item in batch]\n",
    "        }\n",
    "\n",
    "# ============================================================================\n",
    "# Loss Functions\n",
    "# ============================================================================\n",
    "class FocalDiceLoss(nn.Module):\n",
    "    \"\"\"Combined Focal + Dice Loss\"\"\"\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, dice_weight=1.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.dice_weight = dice_weight\n",
    "\n",
    "    def focal_loss(self, pred, target):\n",
    "        bce = F.binary_cross_entropy_with_logits(pred, target, reduction='none')\n",
    "        pt = torch.exp(-bce)\n",
    "        focal = self.alpha * (1 - pt) ** self.gamma * bce\n",
    "        return focal.mean()\n",
    "\n",
    "    def dice_loss(self, pred, target, smooth=1.0):\n",
    "        pred = torch.sigmoid(pred)\n",
    "        pred_flat = pred.view(pred.shape[0], -1)\n",
    "        target_flat = target.view(target.shape[0], -1)\n",
    "\n",
    "        intersection = (pred_flat * target_flat).sum(dim=1)\n",
    "        union = pred_flat.sum(dim=1) + target_flat.sum(dim=1)\n",
    "\n",
    "        dice = (2.0 * intersection + smooth) / (union + smooth)\n",
    "        return 1 - dice.mean()\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        focal = self.focal_loss(pred, target)\n",
    "        dice = self.dice_loss(pred, target)\n",
    "        return focal + self.dice_weight * dice, {\n",
    "            'focal': focal.item(),\n",
    "            'dice': dice.item()\n",
    "        }\n",
    "\n",
    "# ============================================================================\n",
    "# Metrics\n",
    "# ============================================================================\n",
    "def compute_iou(pred, target, threshold=0.5):\n",
    "    pred = (torch.sigmoid(pred) > threshold).float()\n",
    "    intersection = (pred * target).sum()\n",
    "    union = pred.sum() + target.sum() - intersection\n",
    "    return (intersection / (union + 1e-6)).item()\n",
    "\n",
    "\n",
    "def compute_dice(pred, target, threshold=0.5):\n",
    "    pred = (torch.sigmoid(pred) > threshold).float()\n",
    "    intersection = (pred * target).sum()\n",
    "    dice = (2.0 * intersection) / (pred.sum() + target.sum() + 1e-6)\n",
    "    return dice.item()\n",
    "\n",
    "# ============================================================================\n",
    "# Training Functions\n",
    "# ============================================================================\n",
    "\n",
    "def _coerce_logits_to_4d(logits, masks, debug=False):\n",
    "    \"\"\"Coerce logits tensor into shape (N, C, H, W) before interpolation.\"\"\"\n",
    "    # Optional debug print (comment out in production)\n",
    "    if debug:\n",
    "        print(\"DEBUG logits.shape (before reshape):\", tuple(logits.shape))\n",
    "\n",
    "    # If outputs are a HF BatchEncoding-like object, convert to tensor\n",
    "    if isinstance(logits, (list, tuple)):\n",
    "        logits = logits[0]\n",
    "\n",
    "    # Ensure tensor\n",
    "    if not torch.is_tensor(logits):\n",
    "        logits = torch.tensor(logits)\n",
    "\n",
    "    # Typical cases:\n",
    "    # - (N, H, W) -> add channel dim -> (N,1,H,W)\n",
    "    # - (N, C, H, W) -> already fine\n",
    "    # - (N, S) or flattened -> not handled automatically\n",
    "\n",
    "    if logits.dim() == 4:\n",
    "        return logits\n",
    "\n",
    "    if logits.dim() == 3:\n",
    "        # If shape matches masks spatial dims, assume (N, H, W)\n",
    "        N, A, B = logits.shape\n",
    "        if A == masks.shape[-2] and B == masks.shape[-1]:\n",
    "            logits = logits.unsqueeze(1)  # (N,1,H,W)\n",
    "        else:\n",
    "            # Generic: add channel dim\n",
    "            logits = logits.unsqueeze(1)\n",
    "        return logits\n",
    "\n",
    "    if logits.dim() == 2:\n",
    "        # ambiguous: treat as (N, S) -> try to reshape to (N,1,H,W) if possible\n",
    "        N, S = logits.shape\n",
    "        H = masks.shape[-2]\n",
    "        W = masks.shape[-1]\n",
    "        if S == H * W:\n",
    "            logits = logits.view(N, 1, H, W)\n",
    "            return logits\n",
    "        else:\n",
    "            # fallback: add two dims\n",
    "            logits = logits.unsqueeze(1).unsqueeze(-1)\n",
    "            return logits\n",
    "\n",
    "    raise RuntimeError(f\"Unable to coerce logits to 4D; got shape {tuple(logits.shape)}\")\n",
    "\n",
    "\n",
    "def train_epoch(model, loader, criterion, optimizer, scaler, epoch, config):\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    total_iou = 0\n",
    "    total_dice = 0\n",
    "\n",
    "    pbar = tqdm(loader, desc=f'Epoch {epoch} [Train]')\n",
    "\n",
    "    for batch_idx, batch in enumerate(pbar):\n",
    "        pixel_values = batch['pixel_values'].to(config.DEVICE)\n",
    "        input_ids = batch['input_ids'].to(config.DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(config.DEVICE)\n",
    "        masks = batch['masks'].to(config.DEVICE).unsqueeze(1)  # [B,1,H,W]\n",
    "\n",
    "        with autocast(device_type='cuda', dtype=torch.float16, enabled=config.USE_AMP):\n",
    "            outputs = model(\n",
    "                pixel_values=pixel_values,\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask\n",
    "            )\n",
    "\n",
    "            # Defensive retrieval of logits\n",
    "            logits = None\n",
    "            for attr in (\"logits\", \"conditional_logits\"):\n",
    "                if hasattr(outputs, attr):\n",
    "                    logits = getattr(outputs, attr)\n",
    "                    break\n",
    "\n",
    "            if logits is None:\n",
    "                # try dict-like access\n",
    "                try:\n",
    "                    outputs_dict = dict(outputs)\n",
    "                    # pick first tensor-like value\n",
    "                    for v in outputs_dict.values():\n",
    "                        if torch.is_tensor(v):\n",
    "                            logits = v\n",
    "                            break\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            if logits is None:\n",
    "                # Final debug dump\n",
    "                try:\n",
    "                    print(\"Model outputs keys:\", list(outputs.keys()))\n",
    "                except Exception:\n",
    "                    pass\n",
    "                raise RuntimeError(\"No logits-like attribute found on CLIPSeg output.\")\n",
    "\n",
    "            # Coerce logits to 4D\n",
    "            logits = _coerce_logits_to_4d(logits, masks, debug=False)\n",
    "\n",
    "            pred_masks = F.interpolate(\n",
    "                logits,\n",
    "                size=masks.shape[-2:],\n",
    "                mode='bilinear',\n",
    "                align_corners=False\n",
    "            )\n",
    "\n",
    "            loss, loss_dict = criterion(pred_masks, masks)\n",
    "            loss = loss / config.GRADIENT_ACCUMULATION_STEPS\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        if (batch_idx + 1) % config.GRADIENT_ACCUMULATION_STEPS == 0:\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            iou = compute_iou(pred_masks, masks)\n",
    "            dice = compute_dice(pred_masks, masks)\n",
    "\n",
    "        total_loss += loss.item() * config.GRADIENT_ACCUMULATION_STEPS\n",
    "        total_iou += iou\n",
    "        total_dice += dice\n",
    "\n",
    "        pbar.set_postfix({\n",
    "            'loss': f'{loss.item():.4f}',\n",
    "            'iou': f'{iou:.4f}',\n",
    "            'dice': f'{dice:.4f}'\n",
    "        })\n",
    "\n",
    "    n = len(loader)\n",
    "    return total_loss / n, total_iou / n, total_dice / n\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, loader, criterion, config):\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    total_iou = 0\n",
    "    total_dice = 0\n",
    "\n",
    "    for batch in tqdm(loader, desc='Validating'):\n",
    "        pixel_values = batch['pixel_values'].to(config.DEVICE)\n",
    "        input_ids = batch['input_ids'].to(config.DEVICE)\n",
    "        attention_mask = batch['attention_mask'].to(config.DEVICE)\n",
    "        masks = batch['masks'].to(config.DEVICE).unsqueeze(1)\n",
    "\n",
    "        outputs = model(\n",
    "            pixel_values=pixel_values,\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        logits = None\n",
    "        for attr in (\"logits\", \"conditional_logits\"):\n",
    "            if hasattr(outputs, attr):\n",
    "                logits = getattr(outputs, attr)\n",
    "                break\n",
    "\n",
    "        if logits is None:\n",
    "            try:\n",
    "                outputs_dict = dict(outputs)\n",
    "                for v in outputs_dict.values():\n",
    "                    if torch.is_tensor(v):\n",
    "                        logits = v\n",
    "                        break\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        if logits is None:\n",
    "            try:\n",
    "                print(\"Model outputs keys:\", list(outputs.keys()))\n",
    "            except Exception:\n",
    "                pass\n",
    "            raise RuntimeError(\"No logits-like attribute found on CLIPSeg output.\")\n",
    "\n",
    "        logits = _coerce_logits_to_4d(logits, masks, debug=False)\n",
    "\n",
    "        pred_masks = F.interpolate(\n",
    "            logits,\n",
    "            size=masks.shape[-2:],\n",
    "            mode='bilinear',\n",
    "            align_corners=False\n",
    "        )\n",
    "\n",
    "        loss, _ = criterion(pred_masks, masks)\n",
    "        iou = compute_iou(pred_masks, masks)\n",
    "        dice = compute_dice(pred_masks, masks)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_iou += iou\n",
    "        total_dice += dice\n",
    "\n",
    "    n = len(loader)\n",
    "    return total_loss / n, total_iou / n, total_dice / n\n",
    "\n",
    "# ============================================================================\n",
    "# MAIN EXECUTION\n",
    "# ============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=\" * 70)\n",
    "    print(\"CLIPSEG FINE-TUNING\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    print(f\"\\nLoading model: {config.MODEL_NAME}\")\n",
    "    processor = AutoProcessor.from_pretrained(config.MODEL_NAME)\n",
    "\n",
    "    model = CLIPSegForImageSegmentation.from_pretrained(config.MODEL_NAME)\n",
    "    model.to(config.DEVICE)\n",
    "\n",
    "    # Freeze the backbone\n",
    "    freeze_clip_backbone(model)\n",
    "\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "    print(f\"✓ Trainable params: {trainable:,} ({100*trainable/total:.2f}%)\")\n",
    "    print(f\"✓ Frozen params: {total-trainable:,}\")\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = PromptSegDataset(\n",
    "        os.path.join(config.DATASET_DIR, 'train.csv'),\n",
    "        config.DATASET_DIR,\n",
    "        transform=get_transforms(True)\n",
    "    )\n",
    "\n",
    "    valid_dataset = PromptSegDataset(\n",
    "        os.path.join(config.DATASET_DIR, 'valid.csv'),\n",
    "        config.DATASET_DIR,\n",
    "        transform=get_transforms(False)\n",
    "    )\n",
    "\n",
    "    # Create collator\n",
    "    collator = ClipSegCollator(processor)\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=config.NUM_WORKERS,\n",
    "        collate_fn=collator,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=config.BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=config.NUM_WORKERS,\n",
    "        collate_fn=collator,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    print(f\"\\n✓ Train batches: {len(train_loader)}\")\n",
    "    print(f\"✓ Valid batches: {len(valid_loader)}\")\n",
    "\n",
    "    # Setup training\n",
    "    criterion = FocalDiceLoss()\n",
    "    optimizer = optim.AdamW(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()),\n",
    "        lr=config.LEARNING_RATE,\n",
    "        weight_decay=config.WEIGHT_DECAY\n",
    "    )\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer, T_0=5, T_mult=2\n",
    "    )\n",
    "    scaler = GradScaler(device='cuda', enabled=config.USE_AMP)\n",
    "\n",
    "    # Training loop\n",
    "    best_iou = 0\n",
    "    patience_counter = 0\n",
    "    history = {'train_loss': [], 'train_iou': [], 'valid_loss': [], 'valid_iou': []}\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"STARTING TRAINING\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    for epoch in range(1, config.NUM_EPOCHS + 1):\n",
    "        train_loss, train_iou, train_dice = train_epoch(\n",
    "            model, train_loader, criterion, optimizer, scaler, epoch, config\n",
    "        )\n",
    "        valid_loss, valid_iou, valid_dice = validate(\n",
    "            model, valid_loader, criterion, config\n",
    "        )\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_iou'].append(train_iou)\n",
    "        history['valid_loss'].append(valid_loss)\n",
    "        history['valid_iou'].append(valid_iou)\n",
    "\n",
    "        print(f\"\\nEpoch {epoch}\")\n",
    "        print(f\"Train: Loss={train_loss:.4f}, IoU={train_iou:.4f}, Dice={train_dice:.4f}\")\n",
    "        print(f\"Valid: Loss={valid_loss:.4f}, IoU={valid_iou:.4f}, Dice={valid_dice:.4f}\")\n",
    "\n",
    "        # Save best model\n",
    "        if valid_iou > best_iou + config.MIN_DELTA:\n",
    "            best_iou = valid_iou\n",
    "            patience_counter = 0\n",
    "\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'valid_iou': valid_iou,\n",
    "            }, os.path.join(config.SAVE_DIR, 'best_model.pth'))\n",
    "\n",
    "            processor.save_pretrained(config.SAVE_DIR)\n",
    "\n",
    "            print(f\"✅ Best model saved! IoU: {valid_iou:.4f}\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= config.PATIENCE:\n",
    "            print(f\"\\n⚠️ Early stopping triggered!\")\n",
    "            break\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"TRAINING COMPLETE!\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Best validation IoU: {best_iou:.4f}\")\n",
    "    print(f\"Model saved to: {config.SAVE_DIR}/best_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 271467053,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
