{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5fab53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: roboflow in c:\\users\\shiva\\anaconda3\\envs\\gpu\\lib\\site-packages (1.1.49)\n",
      "Requirement already satisfied: certifi in c:\\users\\shiva\\anaconda3\\envs\\gpu\\lib\\site-packages (from roboflow) (2025.10.5)\n",
      "Requirement already satisfied: idna==3.7 in c:\\users\\shiva\\anaconda3\\envs\\gpu\\lib\\site-packages (from roboflow) (3.7)\n",
      "Requirement already satisfied: cycler in c:\\users\\shiva\\anaconda3\\envs\\gpu\\lib\\site-packages (from roboflow) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\shiva\\anaconda3\\envs\\gpu\\lib\\site-packages (from roboflow) (1.4.7)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\shiva\\anaconda3\\envs\\gpu\\lib\\site-packages (from roboflow) (3.10.7)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\shiva\\anaconda3\\envs\\gpu\\lib\\site-packages (from roboflow) (2.3.4)\n",
      "Requirement already satisfied: opencv-python-headless==4.10.0.84 in c:\\users\\shiva\\anaconda3\\envs\\gpu\\lib\\site-packages (from roboflow) (4.10.0.84)\n",
      "Requirement already satisfied: Pillow>=7.1.2 in c:\\users\\shiva\\anaconda3\\envs\\gpu\\lib\\site-packages (from roboflow) (10.4.0)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\shiva\\anaconda3\\envs\\gpu\\lib\\site-packages (from roboflow) (2.9.0.post0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\shiva\\anaconda3\\envs\\gpu\\lib\\site-packages (from roboflow) (1.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\shiva\\anaconda3\\envs\\gpu\\lib\\site-packages (from roboflow) (2.32.5)\n",
      "Requirement already satisfied: six in c:\\users\\shiva\\anaconda3\\envs\\gpu\\lib\\site-packages (from roboflow) (1.16.0)\n",
      "Requirement already satisfied: urllib3>=1.26.6 in c:\\users\\shiva\\anaconda3\\envs\\gpu\\lib\\site-packages (from roboflow) (2.5.0)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in c:\\users\\shiva\\anaconda3\\envs\\gpu\\lib\\site-packages (from roboflow) (4.66.1)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in c:\\users\\shiva\\anaconda3\\envs\\gpu\\lib\\site-packages (from roboflow) (6.0.1)\n",
      "Requirement already satisfied: requests-toolbelt in c:\\users\\shiva\\anaconda3\\envs\\gpu\\lib\\site-packages (from roboflow) (1.0.0)\n",
      "Requirement already satisfied: filetype in c:\\users\\shiva\\anaconda3\\envs\\gpu\\lib\\site-packages (from roboflow) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\shiva\\anaconda3\\envs\\gpu\\lib\\site-packages (from tqdm>=4.41.0->roboflow) (0.4.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\shiva\\anaconda3\\envs\\gpu\\lib\\site-packages (from matplotlib->roboflow) (1.3.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\shiva\\anaconda3\\envs\\gpu\\lib\\site-packages (from matplotlib->roboflow) (4.55.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\shiva\\anaconda3\\envs\\gpu\\lib\\site-packages (from matplotlib->roboflow) (23.2)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\shiva\\anaconda3\\envs\\gpu\\lib\\site-packages (from matplotlib->roboflow) (3.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\shiva\\anaconda3\\envs\\gpu\\lib\\site-packages (from requests->roboflow) (3.3.2)\n",
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in cracks-1 to yolov12:: 100%|██████████| 300888/300888 [06:05<00:00, 823.90it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Dataset Version Zip to cracks-1 in yolov12:: 100%|██████████| 10750/10750 [00:14<00:00, 750.03it/s] \n"
     ]
    }
   ],
   "source": [
    "!pip install roboflow\n",
    "\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"qxxPbuRLdmGnVgKsd0WL\")\n",
    "project = rf.workspace(\"machinelearning-u4gmu\").project(\"cracks-3ii36-jivvr\")\n",
    "version = project.version(1)\n",
    "dataset = version.download(\"yolov12\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70855a8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: roboflow in c:\\users\\shiva\\anaconda3\\envs\\gpu\\lib\\site-packages (1.1.49)\n",
      "Requirement already satisfied: certifi in c:\\users\\shiva\\anaconda3\\envs\\gpu\\lib\\site-packages (from roboflow) (2025.10.5)\n",
      "Requirement already satisfied: idna==3.7 in c:\\users\\shiva\\anaconda3\\envs\\gpu\\lib\\site-packages (from roboflow) (3.7)\n",
      "Requirement already satisfied: cycler in c:\\users\\shiva\\anaconda3\\envs\\gpu\\lib\\site-packages (from roboflow) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\shiva\\anaconda3\\envs\\gpu\\lib\\site-packages (from roboflow) (1.4.7)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\shiva\\anaconda3\\envs\\gpu\\lib\\site-packages (from roboflow) (3.10.7)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\shiva\\anaconda3\\envs\\gpu\\lib\\site-packages (from roboflow) (2.3.4)\n",
      "Requirement already satisfied: opencv-python-headless==4.10.0.84 in c:\\users\\shiva\\anaconda3\\envs\\gpu\\lib\\site-packages (from roboflow) (4.10.0.84)\n",
      "Requirement already satisfied: Pillow>=7.1.2 in c:\\users\\shiva\\anaconda3\\envs\\gpu\\lib\\site-packages (from roboflow) (10.4.0)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\shiva\\anaconda3\\envs\\gpu\\lib\\site-packages (from roboflow) (2.9.0.post0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\shiva\\anaconda3\\envs\\gpu\\lib\\site-packages (from roboflow) (1.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\shiva\\anaconda3\\envs\\gpu\\lib\\site-packages (from roboflow) (2.32.5)\n",
      "Requirement already satisfied: six in c:\\users\\shiva\\anaconda3\\envs\\gpu\\lib\\site-packages (from roboflow) (1.16.0)\n",
      "Requirement already satisfied: urllib3>=1.26.6 in c:\\users\\shiva\\anaconda3\\envs\\gpu\\lib\\site-packages (from roboflow) (2.5.0)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in c:\\users\\shiva\\anaconda3\\envs\\gpu\\lib\\site-packages (from roboflow) (4.66.1)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in c:\\users\\shiva\\anaconda3\\envs\\gpu\\lib\\site-packages (from roboflow) (6.0.1)\n",
      "Requirement already satisfied: requests-toolbelt in c:\\users\\shiva\\anaconda3\\envs\\gpu\\lib\\site-packages (from roboflow) (1.0.0)\n",
      "Requirement already satisfied: filetype in c:\\users\\shiva\\anaconda3\\envs\\gpu\\lib\\site-packages (from roboflow) (1.2.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\shiva\\anaconda3\\envs\\gpu\\lib\\site-packages (from tqdm>=4.41.0->roboflow) (0.4.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\shiva\\anaconda3\\envs\\gpu\\lib\\site-packages (from matplotlib->roboflow) (1.3.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\shiva\\anaconda3\\envs\\gpu\\lib\\site-packages (from matplotlib->roboflow) (4.55.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\shiva\\anaconda3\\envs\\gpu\\lib\\site-packages (from matplotlib->roboflow) (23.2)\n",
      "Requirement already satisfied: pyparsing>=3 in c:\\users\\shiva\\anaconda3\\envs\\gpu\\lib\\site-packages (from matplotlib->roboflow) (3.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\shiva\\anaconda3\\envs\\gpu\\lib\\site-packages (from requests->roboflow) (3.3.2)\n",
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in Drywall-Join-Detect-2 to yolov12:: 100%|██████████| 69930/69930 [01:09<00:00, 1004.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Dataset Version Zip to Drywall-Join-Detect-2 in yolov12:: 100%|██████████| 4983/4983 [00:04<00:00, 1024.35it/s]\n"
     ]
    }
   ],
   "source": [
    "!pip install roboflow\n",
    "\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"qxxPbuRLdmGnVgKsd0WL\")\n",
    "project = rf.workspace(\"machinelearning-u4gmu\").project(\"drywall-join-detect-ioimg\")\n",
    "version = project.version(2)\n",
    "dataset = version.download(\"yolov12\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470c9aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two-Stage Pipeline: YOLOv8 Detection + SAM Segmentation\n",
    "\n",
    "This notebook implements a complete pipeline with two main parts:\n",
    "- **Part A: Training Phase** - Train YOLOv8 on merged datasets (one-time process)\n",
    "- **Part B: Inference Pipeline** - Use trained model + SAM for prompt-based segmentation (user experience)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dbf5ac",
   "metadata": {},
   "source": [
    "## Part A: Training Phase (One-Time Process)\n",
    "\n",
    "### Step 1: Install Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04de8aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ultralytics segment-anything opencv-python matplotlib pyyaml shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093fae3a",
   "metadata": {},
   "source": [
    "### Step 2: Merge Datasets\n",
    "\n",
    "We'll merge the two datasets (cracks-1 and Drywall-Join-Detect-2) into a unified dataset with two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f313b098",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "# Define paths\n",
    "cracks_dataset = 'cracks-1'\n",
    "drywall_dataset = 'Drywall-Join-Detect-2'\n",
    "merged_dataset = 'merged_dataset'\n",
    "\n",
    "# Create merged dataset directory structure\n",
    "for split in ['train', 'valid', 'test']:\n",
    "    os.makedirs(f'{merged_dataset}/{split}/images', exist_ok=True)\n",
    "    os.makedirs(f'{merged_dataset}/{split}/labels', exist_ok=True)\n",
    "\n",
    "# Function to copy and relabel dataset\n",
    "def merge_dataset(source_dir, target_dir, class_id):\n",
    "    \"\"\"\n",
    "    Copy images and labels from source to target, updating class IDs\n",
    "    source_dir: Path to source dataset (e.g., 'cracks-1')\n",
    "    target_dir: Path to merged dataset (e.g., 'merged_dataset')\n",
    "    class_id: New class ID (0 for cracks, 1 for drywall-join)\n",
    "    \"\"\"\n",
    "    for split in ['train', 'valid', 'test']:\n",
    "        # Copy images\n",
    "        src_images = f'{source_dir}/{split}/images'\n",
    "        dst_images = f'{target_dir}/{split}/images'\n",
    "        \n",
    "        if os.path.exists(src_images):\n",
    "            for img_file in os.listdir(src_images):\n",
    "                src_path = os.path.join(src_images, img_file)\n",
    "                dst_path = os.path.join(dst_images, img_file)\n",
    "                if os.path.isfile(src_path):\n",
    "                    shutil.copy2(src_path, dst_path)\n",
    "        \n",
    "        # Copy and update labels\n",
    "        src_labels = f'{source_dir}/{split}/labels'\n",
    "        dst_labels = f'{target_dir}/{split}/labels'\n",
    "        \n",
    "        if os.path.exists(src_labels):\n",
    "            for label_file in os.listdir(src_labels):\n",
    "                src_path = os.path.join(src_labels, label_file)\n",
    "                dst_path = os.path.join(dst_labels, label_file)\n",
    "                \n",
    "                if os.path.isfile(src_path):\n",
    "                    # Read and update class IDs\n",
    "                    with open(src_path, 'r') as f:\n",
    "                        lines = f.readlines()\n",
    "                    \n",
    "                    updated_lines = []\n",
    "                    for line in lines:\n",
    "                        parts = line.strip().split()\n",
    "                        if parts:\n",
    "                            # Update class ID (first element)\n",
    "                            parts[0] = str(class_id)\n",
    "                            updated_lines.append(' '.join(parts) + '\\n')\n",
    "                    \n",
    "                    # Write updated labels\n",
    "                    with open(dst_path, 'w') as f:\n",
    "                        f.writelines(updated_lines)\n",
    "\n",
    "# Merge datasets\n",
    "# Class 0: cracks\n",
    "# Class 1: drywall-join\n",
    "print(\"Merging cracks dataset (class 0)...\")\n",
    "merge_dataset(cracks_dataset, merged_dataset, class_id=0)\n",
    "\n",
    "print(\"Merging drywall-join dataset (class 1)...\")\n",
    "merge_dataset(drywall_dataset, merged_dataset, class_id=1)\n",
    "\n",
    "# Create data.yaml for merged dataset\n",
    "data_config = {\n",
    "    'train': f'../{merged_dataset}/train/images',\n",
    "    'val': f'../{merged_dataset}/valid/images',\n",
    "    'test': f'../{merged_dataset}/test/images',\n",
    "    'nc': 2,\n",
    "    'names': ['crack', 'drywall-join']\n",
    "}\n",
    "\n",
    "with open(f'{merged_dataset}/data.yaml', 'w') as f:\n",
    "    yaml.dump(data_config, f, default_flow_style=False)\n",
    "\n",
    "print(\"\\nDataset merge complete!\")\n",
    "print(f\"Merged dataset location: {merged_dataset}\")\n",
    "print(f\"Classes: {data_config['names']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3705d59",
   "metadata": {},
   "source": [
    "### Step 3: Train YOLOv8 Model\n",
    "\n",
    "Train the YOLOv8 model on the merged dataset. This creates our specialized object detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd85f947",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a pre-trained YOLOv8 model\n",
    "model = YOLO('yolov8n.pt')  # Using nano model for faster training, can use yolov8s.pt, yolov8m.pt for better accuracy\n",
    "\n",
    "# Train the model\n",
    "results = model.train(\n",
    "    data=f'{merged_dataset}/data.yaml',\n",
    "    epochs=100,  # Adjust based on your needs\n",
    "    imgsz=640,\n",
    "    batch=16,\n",
    "    name='crack_drywall_detector',\n",
    "    patience=20,  # Early stopping patience\n",
    "    save=True,\n",
    "    device=0  # Use GPU if available, otherwise set to 'cpu'\n",
    ")\n",
    "\n",
    "print(\"\\nTraining complete!\")\n",
    "print(f\"Best model saved at: runs/detect/crack_drywall_detector/weights/best.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af786c3",
   "metadata": {},
   "source": [
    "### Step 4: Validate the Trained Model\n",
    "\n",
    "Check the performance of the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de12f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best trained model\n",
    "best_model = YOLO('runs/detect/crack_drywall_detector/weights/best.pt')\n",
    "\n",
    "# Validate the model\n",
    "metrics = best_model.val()\n",
    "\n",
    "print(f\"\\nValidation Results:\")\n",
    "print(f\"mAP50: {metrics.box.map50}\")\n",
    "print(f\"mAP50-95: {metrics.box.map}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e6b107",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part B: Inference Pipeline (User Experience)\n",
    "\n",
    "### Step 5: Setup SAM Model\n",
    "\n",
    "Download and initialize the Segment Anything Model (SAM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19f434f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Download SAM checkpoint (run once)\n",
    "# You can download from: https://github.com/facebookresearch/segment-anything#model-checkpoints\n",
    "# For this example, we'll use sam_vit_h_4b8939.pth (ViT-H SAM model)\n",
    "\n",
    "# Initialize SAM\n",
    "sam_checkpoint = \"sam_vit_h_4b8939.pth\"  # Update path if needed\n",
    "model_type = \"vit_h\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device=device)\n",
    "sam_predictor = SamPredictor(sam)\n",
    "\n",
    "print(f\"SAM model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41695d7",
   "metadata": {},
   "source": [
    "### Step 6: Implement Prompt Mapping System\n",
    "\n",
    "Map user prompts to target classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a836eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptMapper:\n",
    "    \"\"\"\n",
    "    Maps user text prompts to specific class names that YOLO understands\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # Define mapping rules for each class\n",
    "        self.mappings = {\n",
    "            'crack': [\n",
    "                'crack', 'cracks', 'fracture', 'fissure', 'split',\n",
    "                'segment crack', 'detect crack', 'find crack',\n",
    "                'show crack', 'identify crack'\n",
    "            ],\n",
    "            'drywall-join': [\n",
    "                'drywall', 'joint', 'join', 'tape', 'taping area',\n",
    "                'seam', 'drywall seam', 'drywall joint', 'joint/tape',\n",
    "                'segment taping area', 'segment joint', 'segment drywall',\n",
    "                'detect joint', 'find tape', 'show seam'\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Flatten for easy lookup\n",
    "        self.prompt_to_class = {}\n",
    "        for class_name, prompts in self.mappings.items():\n",
    "            for prompt in prompts:\n",
    "                self.prompt_to_class[prompt.lower()] = class_name\n",
    "    \n",
    "    def map_prompt(self, user_prompt):\n",
    "        \"\"\"\n",
    "        Convert user prompt to target class\n",
    "        \n",
    "        Args:\n",
    "            user_prompt (str): User's text input\n",
    "            \n",
    "        Returns:\n",
    "            str: Target class name ('crack' or 'drywall-join') or None if no match\n",
    "        \"\"\"\n",
    "        user_prompt = user_prompt.lower().strip()\n",
    "        \n",
    "        # Direct match\n",
    "        if user_prompt in self.prompt_to_class:\n",
    "            return self.prompt_to_class[user_prompt]\n",
    "        \n",
    "        # Partial match - check if any keyword is in the prompt\n",
    "        for keyword, class_name in self.prompt_to_class.items():\n",
    "            if keyword in user_prompt:\n",
    "                return class_name\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def get_class_id(self, class_name):\n",
    "        \"\"\"\n",
    "        Get numeric class ID for a class name\n",
    "        \n",
    "        Args:\n",
    "            class_name (str): Class name ('crack' or 'drywall-join')\n",
    "            \n",
    "        Returns:\n",
    "            int: Class ID (0 for crack, 1 for drywall-join)\n",
    "        \"\"\"\n",
    "        class_mapping = {\n",
    "            'crack': 0,\n",
    "            'drywall-join': 1\n",
    "        }\n",
    "        return class_mapping.get(class_name)\n",
    "\n",
    "# Initialize the prompt mapper\n",
    "prompt_mapper = PromptMapper()\n",
    "\n",
    "# Test the mapper\n",
    "test_prompts = [\n",
    "    \"segment taping area\",\n",
    "    \"segment joint/tape\",\n",
    "    \"segment drywall seam\",\n",
    "    \"find cracks\",\n",
    "    \"detect crack\"\n",
    "]\n",
    "\n",
    "print(\"Prompt Mapping Examples:\")\n",
    "for prompt in test_prompts:\n",
    "    target_class = prompt_mapper.map_prompt(prompt)\n",
    "    class_id = prompt_mapper.get_class_id(target_class)\n",
    "    print(f\"  '{prompt}' -> {target_class} (ID: {class_id})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3e333a",
   "metadata": {},
   "source": [
    "### Step 7: Complete Inference Pipeline\n",
    "\n",
    "Implement the full pipeline: Prompt → Detection → Filtering → Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9231339",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoStagePipeline:\n",
    "    \"\"\"\n",
    "    Complete pipeline combining YOLOv8 detection and SAM segmentation\n",
    "    \"\"\"\n",
    "    def __init__(self, yolo_model_path, sam_predictor, prompt_mapper):\n",
    "        \"\"\"\n",
    "        Initialize the pipeline\n",
    "        \n",
    "        Args:\n",
    "            yolo_model_path (str): Path to trained YOLO model weights\n",
    "            sam_predictor: Initialized SAM predictor\n",
    "            prompt_mapper: Initialized PromptMapper instance\n",
    "        \"\"\"\n",
    "        self.yolo_model = YOLO(yolo_model_path)\n",
    "        self.sam_predictor = sam_predictor\n",
    "        self.prompt_mapper = prompt_mapper\n",
    "        \n",
    "    def process_image(self, image_path, user_prompt, conf_threshold=0.25):\n",
    "        \"\"\"\n",
    "        Main inference pipeline\n",
    "        \n",
    "        Args:\n",
    "            image_path (str): Path to input image\n",
    "            user_prompt (str): User's text prompt\n",
    "            conf_threshold (float): Confidence threshold for YOLO detections\n",
    "            \n",
    "        Returns:\n",
    "            dict: Contains masks, boxes, and visualization\n",
    "        \"\"\"\n",
    "        # Step 1: Map prompt to target class\n",
    "        target_class_name = self.prompt_mapper.map_prompt(user_prompt)\n",
    "        if target_class_name is None:\n",
    "            print(f\"Warning: Could not map prompt '{user_prompt}' to a known class\")\n",
    "            return None\n",
    "        \n",
    "        target_class_id = self.prompt_mapper.get_class_id(target_class_name)\n",
    "        print(f\"Prompt: '{user_prompt}' → Target class: {target_class_name} (ID: {target_class_id})\")\n",
    "        \n",
    "        # Step 2: Load and prepare image\n",
    "        image = cv2.imread(image_path)\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Step 3: Run YOLO detection\n",
    "        results = self.yolo_model(image_path, conf=conf_threshold)[0]\n",
    "        \n",
    "        # Step 4: Filter detections by target class\n",
    "        boxes = []\n",
    "        scores = []\n",
    "        \n",
    "        for box in results.boxes:\n",
    "            class_id = int(box.cls[0])\n",
    "            if class_id == target_class_id:\n",
    "                # Convert to [x1, y1, x2, y2] format\n",
    "                xyxy = box.xyxy[0].cpu().numpy()\n",
    "                boxes.append(xyxy)\n",
    "                scores.append(float(box.conf[0]))\n",
    "        \n",
    "        print(f\"Found {len(boxes)} {target_class_name} detections\")\n",
    "        \n",
    "        if len(boxes) == 0:\n",
    "            print(f\"No {target_class_name} detected in the image\")\n",
    "            return {\n",
    "                'image': image_rgb,\n",
    "                'boxes': [],\n",
    "                'masks': [],\n",
    "                'final_mask': np.zeros(image_rgb.shape[:2], dtype=bool)\n",
    "            }\n",
    "        \n",
    "        # Step 5: Use SAM for segmentation with filtered boxes\n",
    "        self.sam_predictor.set_image(image_rgb)\n",
    "        \n",
    "        all_masks = []\n",
    "        for box in boxes:\n",
    "            # SAM expects boxes in xyxy format\n",
    "            mask, _, _ = self.sam_predictor.predict(\n",
    "                box=box,\n",
    "                multimask_output=False\n",
    "            )\n",
    "            all_masks.append(mask[0])\n",
    "        \n",
    "        # Step 6: Merge all masks into final binary mask\n",
    "        final_mask = np.zeros(image_rgb.shape[:2], dtype=bool)\n",
    "        for mask in all_masks:\n",
    "            final_mask = final_mask | mask\n",
    "        \n",
    "        print(f\"Generated {len(all_masks)} segmentation masks\")\n",
    "        \n",
    "        return {\n",
    "            'image': image_rgb,\n",
    "            'boxes': boxes,\n",
    "            'masks': all_masks,\n",
    "            'final_mask': final_mask,\n",
    "            'target_class': target_class_name,\n",
    "            'scores': scores\n",
    "        }\n",
    "    \n",
    "    def visualize_results(self, results, figsize=(15, 5)):\n",
    "        \"\"\"\n",
    "        Visualize the pipeline results\n",
    "        \n",
    "        Args:\n",
    "            results (dict): Output from process_image\n",
    "            figsize (tuple): Figure size for visualization\n",
    "        \"\"\"\n",
    "        if results is None:\n",
    "            print(\"No results to visualize\")\n",
    "            return\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 3, figsize=figsize)\n",
    "        \n",
    "        # Original image with bounding boxes\n",
    "        ax = axes[0]\n",
    "        ax.imshow(results['image'])\n",
    "        for i, box in enumerate(results['boxes']):\n",
    "            x1, y1, x2, y2 = box\n",
    "            rect = plt.Rectangle((x1, y1), x2-x1, y2-y1, \n",
    "                                fill=False, edgecolor='red', linewidth=2)\n",
    "            ax.add_patch(rect)\n",
    "            ax.text(x1, y1-5, f\"{results['target_class']} {results['scores'][i]:.2f}\", \n",
    "                   color='red', fontsize=10, weight='bold',\n",
    "                   bbox=dict(facecolor='white', alpha=0.7))\n",
    "        ax.set_title('YOLO Detections (Filtered)')\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Individual masks\n",
    "        ax = axes[1]\n",
    "        ax.imshow(results['image'])\n",
    "        for mask in results['masks']:\n",
    "            colored_mask = np.zeros_like(results['image'])\n",
    "            colored_mask[mask] = [255, 0, 0]  # Red color for masks\n",
    "            ax.imshow(colored_mask, alpha=0.5)\n",
    "        ax.set_title('SAM Segmentation Masks')\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Final merged mask\n",
    "        ax = axes[2]\n",
    "        ax.imshow(results['image'])\n",
    "        colored_final = np.zeros_like(results['image'])\n",
    "        colored_final[results['final_mask']] = [0, 255, 0]  # Green color\n",
    "        ax.imshow(colored_final, alpha=0.6)\n",
    "        ax.set_title('Final Merged Mask')\n",
    "        ax.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Initialize the complete pipeline\n",
    "pipeline = TwoStagePipeline(\n",
    "    yolo_model_path='runs/detect/crack_drywall_detector/weights/best.pt',\n",
    "    sam_predictor=sam_predictor,\n",
    "    prompt_mapper=prompt_mapper\n",
    ")\n",
    "\n",
    "print(\"Pipeline initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dd2232",
   "metadata": {},
   "source": [
    "### Step 8: Test the Pipeline\n",
    "\n",
    "Run the pipeline with different prompts on test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1fcb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Segment taping area\n",
    "test_image_path = 'path/to/your/test/image.jpg'  # Update with actual path\n",
    "user_prompt = \"segment taping area\"\n",
    "\n",
    "results = pipeline.process_image(test_image_path, user_prompt, conf_threshold=0.3)\n",
    "pipeline.visualize_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be46649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: Find cracks\n",
    "user_prompt = \"find cracks\"\n",
    "\n",
    "results = pipeline.process_image(test_image_path, user_prompt, conf_threshold=0.3)\n",
    "pipeline.visualize_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71303d46",
   "metadata": {},
   "source": [
    "### Step 9: Batch Processing\n",
    "\n",
    "Process multiple images with the same or different prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293c960f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def batch_process(image_folder, prompts, output_folder='results', conf_threshold=0.3):\n",
    "    \"\"\"\n",
    "    Process multiple images with different prompts\n",
    "    \n",
    "    Args:\n",
    "        image_folder (str): Folder containing images\n",
    "        prompts (list): List of prompts to try on each image\n",
    "        output_folder (str): Where to save results\n",
    "        conf_threshold (float): Confidence threshold\n",
    "    \"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # Get all images\n",
    "    image_extensions = ['*.jpg', '*.jpeg', '*.png']\n",
    "    image_paths = []\n",
    "    for ext in image_extensions:\n",
    "        image_paths.extend(glob.glob(os.path.join(image_folder, ext)))\n",
    "    \n",
    "    print(f\"Found {len(image_paths)} images\")\n",
    "    \n",
    "    for img_path in image_paths:\n",
    "        img_name = os.path.basename(img_path)\n",
    "        print(f\"\\nProcessing: {img_name}\")\n",
    "        \n",
    "        for prompt in prompts:\n",
    "            results = pipeline.process_image(img_path, prompt, conf_threshold)\n",
    "            \n",
    "            if results and results['final_mask'].any():\n",
    "                # Save the final mask\n",
    "                output_name = f\"{os.path.splitext(img_name)[0]}_{results['target_class']}_mask.png\"\n",
    "                output_path = os.path.join(output_folder, output_name)\n",
    "                \n",
    "                # Convert mask to image\n",
    "                mask_img = (results['final_mask'] * 255).astype(np.uint8)\n",
    "                cv2.imwrite(output_path, mask_img)\n",
    "                print(f\"  Saved: {output_name}\")\n",
    "\n",
    "# Example batch processing\n",
    "# Uncomment and update paths to use\n",
    "\"\"\"\n",
    "batch_process(\n",
    "    image_folder='merged_dataset/test/images',\n",
    "    prompts=['segment taping area', 'find cracks'],\n",
    "    output_folder='pipeline_results'\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efc3aef",
   "metadata": {},
   "source": [
    "### Step 10: Export Final Mask Function\n",
    "\n",
    "Utility function to save masks in different formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f508d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_mask(results, output_path, format='binary'):\n",
    "    \"\"\"\n",
    "    Export segmentation mask in various formats\n",
    "    \n",
    "    Args:\n",
    "        results (dict): Pipeline results\n",
    "        output_path (str): Output file path\n",
    "        format (str): 'binary', 'overlay', or 'colored'\n",
    "    \"\"\"\n",
    "    if results is None or not results['final_mask'].any():\n",
    "        print(\"No mask to export\")\n",
    "        return\n",
    "    \n",
    "    if format == 'binary':\n",
    "        # Binary mask (0 or 255)\n",
    "        mask_img = (results['final_mask'] * 255).astype(np.uint8)\n",
    "        cv2.imwrite(output_path, mask_img)\n",
    "        \n",
    "    elif format == 'overlay':\n",
    "        # Original image with colored overlay\n",
    "        overlay = results['image'].copy()\n",
    "        colored_mask = np.zeros_like(overlay)\n",
    "        colored_mask[results['final_mask']] = [0, 255, 0]  # Green\n",
    "        overlay = cv2.addWeighted(overlay, 0.7, colored_mask, 0.3, 0)\n",
    "        cv2.imwrite(output_path, cv2.cvtColor(overlay, cv2.COLOR_RGB2BGR))\n",
    "        \n",
    "    elif format == 'colored':\n",
    "        # Just the colored mask\n",
    "        colored_mask = np.zeros_like(results['image'])\n",
    "        colored_mask[results['final_mask']] = [0, 255, 0]\n",
    "        cv2.imwrite(output_path, cv2.cvtColor(colored_mask, cv2.COLOR_RGB2BGR))\n",
    "    \n",
    "    print(f\"Mask exported to: {output_path}\")\n",
    "\n",
    "# Example usage\n",
    "\"\"\"\n",
    "results = pipeline.process_image('test.jpg', 'segment taping area')\n",
    "export_mask(results, 'output_binary.png', format='binary')\n",
    "export_mask(results, 'output_overlay.png', format='overlay')\n",
    "export_mask(results, 'output_colored.png', format='colored')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d14adbc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary and Usage Guide\n",
    "\n",
    "### Pipeline Architecture\n",
    "\n",
    "**Part A - Training Phase (One-Time)**:\n",
    "1. ✅ Download datasets from Roboflow (cracks-1, Drywall-Join-Detect-2)\n",
    "2. ✅ Merge datasets with unified class labels (crack=0, drywall-join=1)\n",
    "3. ✅ Train YOLOv8 model on merged dataset\n",
    "4. ✅ Save best weights (`best.pt`)\n",
    "\n",
    "**Part B - Inference Pipeline (Real-Time)**:\n",
    "1. ✅ User provides image + text prompt\n",
    "2. ✅ Prompt mapper translates text → target class\n",
    "3. ✅ YOLO detects all objects → filters by target class\n",
    "4. ✅ Filtered boxes → SAM generates pixel-level masks\n",
    "5. ✅ Merge all masks → return final binary mask\n",
    "\n",
    "### Quick Start\n",
    "\n",
    "```python\n",
    "# After training, use the pipeline:\n",
    "results = pipeline.process_image(\n",
    "    image_path='your_image.jpg',\n",
    "    user_prompt='segment taping area',  # or 'find cracks'\n",
    "    conf_threshold=0.3\n",
    ")\n",
    "\n",
    "# Visualize\n",
    "pipeline.visualize_results(results)\n",
    "\n",
    "# Export\n",
    "export_mask(results, 'output.png', format='overlay')\n",
    "```\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Flexible Prompting**: Multiple ways to describe same object (e.g., \"taping area\", \"drywall joint\", \"seam\")\n",
    "- **Two-Class Detection**: Handles both cracks and drywall joints\n",
    "- **Precise Segmentation**: SAM provides pixel-perfect masks\n",
    "- **Batch Processing**: Process multiple images efficiently\n",
    "- **Multiple Export Formats**: Binary, overlay, or colored masks\n",
    "\n",
    "### Important Notes\n",
    "\n",
    "1. **SAM Model**: Download SAM checkpoint from [here](https://github.com/facebookresearch/segment-anything#model-checkpoints)\n",
    "2. **GPU Recommended**: Both training and inference benefit from GPU acceleration\n",
    "3. **Confidence Threshold**: Adjust `conf_threshold` based on your precision/recall needs\n",
    "4. **Custom Prompts**: Add new prompt mappings in `PromptMapper` class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b23b88",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📊 Model Evaluation: mIoU & Dice Score\n",
    "\n",
    "This section evaluates the segmentation quality using two key metrics:\n",
    "- **mIoU (mean Intersection over Union)**: Measures overlap between predicted and ground truth masks\n",
    "- **Dice Score**: Harmonic mean of precision and recall (2×overlap / total pixels)\n",
    "\n",
    "We'll evaluate using **two different prompt types**:\n",
    "1. **Bounding Box Prompts**: Direct ground truth boxes → SAM (ideal scenario)\n",
    "2. **Text Prompts**: Natural language → YOLO → SAM (realistic user scenario)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f33c298",
   "metadata": {},
   "source": [
    "### Step 11: Metric Calculation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5945ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(pred_mask, gt_mask):\n",
    "    \"\"\"\n",
    "    Calculate Intersection over Union (IoU)\n",
    "    \n",
    "    Args:\n",
    "        pred_mask (np.array): Predicted binary mask\n",
    "        gt_mask (np.array): Ground truth binary mask\n",
    "        \n",
    "    Returns:\n",
    "        float: IoU score (0 to 1)\n",
    "    \"\"\"\n",
    "    intersection = np.logical_and(pred_mask, gt_mask).sum()\n",
    "    union = np.logical_or(pred_mask, gt_mask).sum()\n",
    "    \n",
    "    if union == 0:\n",
    "        return 1.0 if intersection == 0 else 0.0\n",
    "    \n",
    "    return intersection / union\n",
    "\n",
    "\n",
    "def calculate_dice(pred_mask, gt_mask):\n",
    "    \"\"\"\n",
    "    Calculate Dice coefficient (F1 score for segmentation)\n",
    "    \n",
    "    Args:\n",
    "        pred_mask (np.array): Predicted binary mask\n",
    "        gt_mask (np.array): Ground truth binary mask\n",
    "        \n",
    "    Returns:\n",
    "        float: Dice score (0 to 1)\n",
    "    \"\"\"\n",
    "    intersection = np.logical_and(pred_mask, gt_mask).sum()\n",
    "    pred_sum = pred_mask.sum()\n",
    "    gt_sum = gt_mask.sum()\n",
    "    \n",
    "    if (pred_sum + gt_sum) == 0:\n",
    "        return 1.0 if intersection == 0 else 0.0\n",
    "    \n",
    "    return (2.0 * intersection) / (pred_sum + gt_sum)\n",
    "\n",
    "\n",
    "def yolo_to_bbox(label_line, img_width, img_height):\n",
    "    \"\"\"\n",
    "    Convert YOLO format (normalized) to pixel bounding box coordinates\n",
    "    \n",
    "    Args:\n",
    "        label_line (str): YOLO format line (class x_center y_center width height)\n",
    "        img_width (int): Image width in pixels\n",
    "        img_height (int): Image height in pixels\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (class_id, x1, y1, x2, y2) in pixel coordinates\n",
    "    \"\"\"\n",
    "    parts = label_line.strip().split()\n",
    "    class_id = int(parts[0])\n",
    "    x_center = float(parts[1]) * img_width\n",
    "    y_center = float(parts[2]) * img_height\n",
    "    width = float(parts[3]) * img_width\n",
    "    height = float(parts[4]) * img_height\n",
    "    \n",
    "    x1 = max(0, int(x_center - width / 2))\n",
    "    y1 = max(0, int(y_center - height / 2))\n",
    "    x2 = min(img_width, int(x_center + width / 2))\n",
    "    y2 = min(img_height, int(y_center + height / 2))\n",
    "    \n",
    "    return class_id, x1, y1, x2, y2\n",
    "\n",
    "\n",
    "def create_mask_from_bbox(bbox, img_shape):\n",
    "    \"\"\"\n",
    "    Create a binary mask from bounding box (used as ground truth approximation)\n",
    "    \n",
    "    Args:\n",
    "        bbox (tuple): (x1, y1, x2, y2) in pixels\n",
    "        img_shape (tuple): (height, width)\n",
    "        \n",
    "    Returns:\n",
    "        np.array: Binary mask\n",
    "    \"\"\"\n",
    "    mask = np.zeros(img_shape, dtype=bool)\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    mask[y1:y2, x1:x2] = True\n",
    "    return mask\n",
    "\n",
    "print(\"✓ Metric calculation functions loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a804fa4",
   "metadata": {},
   "source": [
    "### Step 12: Evaluation Method 1 - SAM with Bounding Box Prompts\n",
    "\n",
    "**How it works:**\n",
    "1. Load ground truth bounding boxes from YOLO labels\n",
    "2. Use these boxes directly as prompts for SAM\n",
    "3. Compare SAM's predicted masks with ground truth boxes\n",
    "4. Calculate mIoU and Dice scores\n",
    "\n",
    "This represents the **best-case scenario** - SAM gets perfect bounding box prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c662c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_sam_with_bbox_prompts(test_images_dir, test_labels_dir, sam_predictor, max_samples=None):\n",
    "    \"\"\"\n",
    "    Evaluate SAM using ground truth bounding boxes as prompts\n",
    "    \n",
    "    Args:\n",
    "        test_images_dir (str): Path to test images folder\n",
    "        test_labels_dir (str): Path to test labels folder\n",
    "        sam_predictor: Initialized SAM predictor\n",
    "        max_samples (int): Max images to evaluate (None = all)\n",
    "        \n",
    "    Returns:\n",
    "        dict: Evaluation metrics\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EVALUATION METHOD 1: SAM with Ground Truth Bounding Box Prompts\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Get all test images\n",
    "    image_files = sorted([f for f in os.listdir(test_images_dir) \n",
    "                         if f.endswith(('.jpg', '.jpeg', '.png'))])\n",
    "    \n",
    "    if max_samples:\n",
    "        image_files = image_files[:max_samples]\n",
    "    \n",
    "    # Storage for metrics\n",
    "    all_iou_scores = []\n",
    "    all_dice_scores = []\n",
    "    class_metrics = {\n",
    "        0: {'iou': [], 'dice': [], 'name': 'crack'},\n",
    "        1: {'iou': [], 'dice': [], 'name': 'drywall-join'}\n",
    "    }\n",
    "    \n",
    "    processed_count = 0\n",
    "    \n",
    "    for idx, img_file in enumerate(image_files):\n",
    "        # Load image\n",
    "        img_path = os.path.join(test_images_dir, img_file)\n",
    "        image = cv2.imread(img_path)\n",
    "        if image is None:\n",
    "            continue\n",
    "            \n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        img_height, img_width = image.shape[:2]\n",
    "        \n",
    "        # Load corresponding label\n",
    "        label_file = os.path.splitext(img_file)[0] + '.txt'\n",
    "        label_path = os.path.join(test_labels_dir, label_file)\n",
    "        \n",
    "        if not os.path.exists(label_path):\n",
    "            continue\n",
    "        \n",
    "        # Set image for SAM\n",
    "        sam_predictor.set_image(image_rgb)\n",
    "        \n",
    "        # Read all bounding boxes from label file\n",
    "        with open(label_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        if not lines:\n",
    "            continue\n",
    "            \n",
    "        processed_count += 1\n",
    "        \n",
    "        for line in lines:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            \n",
    "            # Parse YOLO format to get bbox\n",
    "            class_id, x1, y1, x2, y2 = yolo_to_bbox(line, img_width, img_height)\n",
    "            \n",
    "            # Create ground truth mask from bbox\n",
    "            gt_mask = create_mask_from_bbox((x1, y1, x2, y2), (img_height, img_width))\n",
    "            \n",
    "            # Use SAM with ground truth bbox as prompt\n",
    "            bbox_array = np.array([x1, y1, x2, y2])\n",
    "            try:\n",
    "                pred_mask, _, _ = sam_predictor.predict(\n",
    "                    box=bbox_array,\n",
    "                    multimask_output=False\n",
    "                )\n",
    "                pred_mask = pred_mask[0]\n",
    "                \n",
    "                # Calculate metrics\n",
    "                iou = calculate_iou(pred_mask, gt_mask)\n",
    "                dice = calculate_dice(pred_mask, gt_mask)\n",
    "                \n",
    "                all_iou_scores.append(iou)\n",
    "                all_dice_scores.append(dice)\n",
    "                class_metrics[class_id]['iou'].append(iou)\n",
    "                class_metrics[class_id]['dice'].append(dice)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {img_file}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Progress update\n",
    "        if (idx + 1) % 10 == 0:\n",
    "            print(f\"  Processed {idx + 1}/{len(image_files)} images...\")\n",
    "    \n",
    "    # Calculate results\n",
    "    results = {\n",
    "        'method': 'BBox Prompts',\n",
    "        'total_images': processed_count,\n",
    "        'total_instances': len(all_iou_scores),\n",
    "        'mean_iou': np.mean(all_iou_scores) if all_iou_scores else 0.0,\n",
    "        'mean_dice': np.mean(all_dice_scores) if all_dice_scores else 0.0,\n",
    "        'std_iou': np.std(all_iou_scores) if all_iou_scores else 0.0,\n",
    "        'std_dice': np.std(all_dice_scores) if all_dice_scores else 0.0,\n",
    "    }\n",
    "    \n",
    "    # Per-class metrics\n",
    "    for class_id in [0, 1]:\n",
    "        class_name = class_metrics[class_id]['name']\n",
    "        results[f'{class_name}_iou'] = np.mean(class_metrics[class_id]['iou']) if class_metrics[class_id]['iou'] else 0.0\n",
    "        results[f'{class_name}_dice'] = np.mean(class_metrics[class_id]['dice']) if class_metrics[class_id]['dice'] else 0.0\n",
    "        results[f'{class_name}_count'] = len(class_metrics[class_id]['iou'])\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"RESULTS:\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Images Processed: {results['total_images']}\")\n",
    "    print(f\"Total Instances: {results['total_instances']}\")\n",
    "    print(f\"\\n📊 Overall Metrics:\")\n",
    "    print(f\"  Mean IoU:  {results['mean_iou']:.4f} (±{results['std_iou']:.4f})\")\n",
    "    print(f\"  Mean Dice: {results['mean_dice']:.4f} (±{results['std_dice']:.4f})\")\n",
    "    print(f\"\\n📊 Per-Class Metrics:\")\n",
    "    print(f\"  Crack (n={results['crack_count']}):\")\n",
    "    print(f\"    IoU:  {results['crack_iou']:.4f}\")\n",
    "    print(f\"    Dice: {results['crack_dice']:.4f}\")\n",
    "    print(f\"  Drywall-Join (n={results['drywall-join_count']}):\")\n",
    "    print(f\"    IoU:  {results['drywall-join_iou']:.4f}\")\n",
    "    print(f\"    Dice: {results['drywall-join_dice']:.4f}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run evaluation with bounding box prompts\n",
    "bbox_results = evaluate_sam_with_bbox_prompts(\n",
    "    test_images_dir='merged_dataset/test/images',\n",
    "    test_labels_dir='merged_dataset/test/labels',\n",
    "    sam_predictor=sam_predictor,\n",
    "    max_samples=50  # Set to None to evaluate all test images\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0bad15",
   "metadata": {},
   "source": [
    "### Step 13: Evaluation Method 2 - Complete Pipeline with Text Prompts\n",
    "\n",
    "**How it works:**\n",
    "1. User provides natural language text prompt (e.g., \"find cracks\")\n",
    "2. Prompt mapper converts text → target class\n",
    "3. YOLO model detects objects and filters by target class\n",
    "4. SAM uses YOLO's predicted boxes to generate masks\n",
    "5. Compare final masks with ground truth\n",
    "6. Calculate mIoU and Dice scores\n",
    "\n",
    "This represents the **real-world scenario** - end-to-end pipeline performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3fe0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pipeline_with_text_prompts(test_images_dir, test_labels_dir, pipeline, \n",
    "                                        prompts_per_class, conf_threshold=0.25, max_samples=None):\n",
    "    \"\"\"\n",
    "    Evaluate complete pipeline (YOLO + SAM) using text prompts\n",
    "    \n",
    "    Args:\n",
    "        test_images_dir (str): Path to test images folder\n",
    "        test_labels_dir (str): Path to test labels folder\n",
    "        pipeline: Initialized TwoStagePipeline instance\n",
    "        prompts_per_class (dict): Dictionary mapping class_id to list of text prompts\n",
    "        conf_threshold (float): YOLO confidence threshold\n",
    "        max_samples (int): Max images to evaluate (None = all)\n",
    "        \n",
    "    Returns:\n",
    "        dict: Evaluation metrics\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"EVALUATION METHOD 2: Complete Pipeline with Text Prompts\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Get all test images\n",
    "    image_files = sorted([f for f in os.listdir(test_images_dir) \n",
    "                         if f.endswith(('.jpg', '.jpeg', '.png'))])\n",
    "    \n",
    "    if max_samples:\n",
    "        image_files = image_files[:max_samples]\n",
    "    \n",
    "    # Storage for metrics per prompt\n",
    "    prompt_results = {}\n",
    "    \n",
    "    # Process each class and its prompts\n",
    "    for class_id, prompts in prompts_per_class.items():\n",
    "        class_name = 'crack' if class_id == 0 else 'drywall-join'\n",
    "        \n",
    "        for prompt in prompts:\n",
    "            print(f\"\\n📝 Testing prompt: '{prompt}' (class: {class_name})\")\n",
    "            \n",
    "            prompt_iou = []\n",
    "            prompt_dice = []\n",
    "            processed_count = 0\n",
    "            matched_instances = 0\n",
    "            \n",
    "            for idx, img_file in enumerate(image_files):\n",
    "                # Load image\n",
    "                img_path = os.path.join(test_images_dir, img_file)\n",
    "                image = cv2.imread(img_path)\n",
    "                if image is None:\n",
    "                    continue\n",
    "                    \n",
    "                image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                img_height, img_width = image.shape[:2]\n",
    "                \n",
    "                # Load ground truth labels\n",
    "                label_file = os.path.splitext(img_file)[0] + '.txt'\n",
    "                label_path = os.path.join(test_labels_dir, label_file)\n",
    "                \n",
    "                if not os.path.exists(label_path):\n",
    "                    continue\n",
    "                \n",
    "                # Read ground truth boxes for this class\n",
    "                gt_boxes_for_class = []\n",
    "                with open(label_path, 'r') as f:\n",
    "                    for line in f:\n",
    "                        if not line.strip():\n",
    "                            continue\n",
    "                        gt_class_id, x1, y1, x2, y2 = yolo_to_bbox(line, img_width, img_height)\n",
    "                        if gt_class_id == class_id:\n",
    "                            gt_boxes_for_class.append((x1, y1, x2, y2))\n",
    "                \n",
    "                if not gt_boxes_for_class:\n",
    "                    continue  # No instances of this class in image\n",
    "                \n",
    "                processed_count += 1\n",
    "                \n",
    "                # Run pipeline with text prompt\n",
    "                try:\n",
    "                    results = pipeline.process_image(img_path, prompt, conf_threshold)\n",
    "                    \n",
    "                    if results is None or not results['final_mask'].any():\n",
    "                        continue  # No detections\n",
    "                    \n",
    "                    # Create combined ground truth mask for this class\n",
    "                    gt_combined_mask = np.zeros((img_height, img_width), dtype=bool)\n",
    "                    for bbox in gt_boxes_for_class:\n",
    "                        gt_mask = create_mask_from_bbox(bbox, (img_height, img_width))\n",
    "                        gt_combined_mask = np.logical_or(gt_combined_mask, gt_mask)\n",
    "                    \n",
    "                    # Calculate metrics against combined mask\n",
    "                    iou = calculate_iou(results['final_mask'], gt_combined_mask)\n",
    "                    dice = calculate_dice(results['final_mask'], gt_combined_mask)\n",
    "                    \n",
    "                    prompt_iou.append(iou)\n",
    "                    prompt_dice.append(dice)\n",
    "                    matched_instances += len(gt_boxes_for_class)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"    Error on {img_file}: {e}\")\n",
    "                    continue\n",
    "                \n",
    "                # Progress update\n",
    "                if (idx + 1) % 20 == 0:\n",
    "                    print(f\"    Processed {idx + 1}/{len(image_files)} images...\")\n",
    "            \n",
    "            # Store results for this prompt\n",
    "            prompt_results[prompt] = {\n",
    "                'class_id': class_id,\n",
    "                'class_name': class_name,\n",
    "                'images_processed': processed_count,\n",
    "                'instances_matched': matched_instances,\n",
    "                'mean_iou': np.mean(prompt_iou) if prompt_iou else 0.0,\n",
    "                'mean_dice': np.mean(prompt_dice) if prompt_dice else 0.0,\n",
    "                'std_iou': np.std(prompt_iou) if prompt_iou else 0.0,\n",
    "                'std_dice': np.std(prompt_dice) if prompt_dice else 0.0,\n",
    "            }\n",
    "            \n",
    "            print(f\"    ✓ Results: IoU={prompt_results[prompt]['mean_iou']:.4f}, \"\n",
    "                  f\"Dice={prompt_results[prompt]['mean_dice']:.4f} \"\n",
    "                  f\"({processed_count} images, {matched_instances} instances)\")\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    all_iou = [r['mean_iou'] for r in prompt_results.values() if r['mean_iou'] > 0]\n",
    "    all_dice = [r['mean_dice'] for r in prompt_results.values() if r['mean_dice'] > 0]\n",
    "    \n",
    "    overall_results = {\n",
    "        'method': 'Text Prompts (Pipeline)',\n",
    "        'num_prompts': len(prompt_results),\n",
    "        'overall_mean_iou': np.mean(all_iou) if all_iou else 0.0,\n",
    "        'overall_mean_dice': np.mean(all_dice) if all_dice else 0.0,\n",
    "        'prompt_details': prompt_results\n",
    "    }\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"RESULTS SUMMARY:\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"Total Prompts Tested: {len(prompt_results)}\")\n",
    "    print(f\"\\n📊 Overall Metrics (averaged across all prompts):\")\n",
    "    print(f\"  Mean IoU:  {overall_results['overall_mean_iou']:.4f}\")\n",
    "    print(f\"  Mean Dice: {overall_results['overall_mean_dice']:.4f}\")\n",
    "    print(f\"\\n📊 Per-Prompt Results:\")\n",
    "    for prompt, res in prompt_results.items():\n",
    "        print(f\"\\n  '{prompt}' ({res['class_name']}):\")\n",
    "        print(f\"    Images: {res['images_processed']}, Instances: {res['instances_matched']}\")\n",
    "        print(f\"    IoU:  {res['mean_iou']:.4f} (±{res['std_iou']:.4f})\")\n",
    "        print(f\"    Dice: {res['mean_dice']:.4f} (±{res['std_dice']:.4f})\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return overall_results\n",
    "\n",
    "# Define test prompts for each class\n",
    "test_prompts = {\n",
    "    0: ['find cracks', 'detect crack', 'segment crack'],  # Class 0: crack\n",
    "    1: ['segment taping area', 'segment joint', 'detect drywall']  # Class 1: drywall-join\n",
    "}\n",
    "\n",
    "# Run evaluation with text prompts\n",
    "text_results = evaluate_pipeline_with_text_prompts(\n",
    "    test_images_dir='merged_dataset/test/images',\n",
    "    test_labels_dir='merged_dataset/test/labels',\n",
    "    pipeline=pipeline,\n",
    "    prompts_per_class=test_prompts,\n",
    "    conf_threshold=0.3,\n",
    "    max_samples=50  # Set to None to evaluate all test images\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc00d492",
   "metadata": {},
   "source": [
    "### Step 14: Comparison and Visualization\n",
    "\n",
    "Compare both evaluation methods side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c787c863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def compare_evaluation_methods(bbox_results, text_results):\n",
    "    \"\"\"\n",
    "    Create a comparison table and visualization for both evaluation methods\n",
    "    \n",
    "    Args:\n",
    "        bbox_results (dict): Results from bounding box prompt evaluation\n",
    "        text_results (dict): Results from text prompt evaluation\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"COMPARATIVE ANALYSIS: BBox Prompts vs Text Prompts\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Create comparison dataframe\n",
    "    comparison_data = {\n",
    "        'Method': [\n",
    "            'SAM with BBox Prompts',\n",
    "            'Pipeline with Text Prompts'\n",
    "        ],\n",
    "        'Mean IoU': [\n",
    "            bbox_results['mean_iou'],\n",
    "            text_results['overall_mean_iou']\n",
    "        ],\n",
    "        'Mean Dice': [\n",
    "            bbox_results['mean_dice'],\n",
    "            text_results['overall_mean_dice']\n",
    "        ],\n",
    "        'Instances': [\n",
    "            bbox_results['total_instances'],\n",
    "            sum([r['instances_matched'] for r in text_results['prompt_details'].values()])\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    df_comparison = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    print(\"\\n📊 Overall Comparison:\")\n",
    "    print(df_comparison.to_string(index=False))\n",
    "    \n",
    "    # Performance difference\n",
    "    iou_diff = bbox_results['mean_iou'] - text_results['overall_mean_iou']\n",
    "    dice_diff = bbox_results['mean_dice'] - text_results['overall_mean_dice']\n",
    "    \n",
    "    print(f\"\\n📉 Performance Gap:\")\n",
    "    print(f\"  IoU difference:  {iou_diff:+.4f} ({iou_diff/bbox_results['mean_iou']*100:+.2f}%)\")\n",
    "    print(f\"  Dice difference: {dice_diff:+.4f} ({dice_diff/bbox_results['mean_dice']*100:+.2f}%)\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # IoU comparison\n",
    "    ax = axes[0]\n",
    "    methods = ['BBox\\nPrompts', 'Text\\nPrompts']\n",
    "    iou_scores = [bbox_results['mean_iou'], text_results['overall_mean_iou']]\n",
    "    bars = ax.bar(methods, iou_scores, color=['#2ecc71', '#3498db'], alpha=0.8, edgecolor='black')\n",
    "    ax.set_ylabel('Mean IoU Score', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Mean IoU Comparison', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylim([0, 1.0])\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, score in zip(bars, iou_scores):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{score:.4f}',\n",
    "                ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "    \n",
    "    # Dice comparison\n",
    "    ax = axes[1]\n",
    "    dice_scores = [bbox_results['mean_dice'], text_results['overall_mean_dice']]\n",
    "    bars = ax.bar(methods, dice_scores, color=['#e74c3c', '#9b59b6'], alpha=0.8, edgecolor='black')\n",
    "    ax.set_ylabel('Mean Dice Score', fontsize=12, fontweight='bold')\n",
    "    ax.set_title('Mean Dice Comparison', fontsize=14, fontweight='bold')\n",
    "    ax.set_ylim([0, 1.0])\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, score in zip(bars, dice_scores):\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{score:.4f}',\n",
    "                ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"KEY INSIGHTS:\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"1. BBox Prompts: Upper bound performance (SAM with perfect inputs)\")\n",
    "    print(\"2. Text Prompts: Real-world performance (includes YOLO detection errors)\")\n",
    "    print(\"3. Gap indicates YOLO detection quality impact on final segmentation\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "# Run comparison\n",
    "compare_evaluation_methods(bbox_results, text_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb663c0",
   "metadata": {},
   "source": [
    "### Step 15: Visual Sample Comparison\n",
    "\n",
    "Show side-by-side predictions from both methods on sample images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e10f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_sample_predictions(image_path, label_path, sam_predictor, pipeline, \n",
    "                                 text_prompt, conf_threshold=0.3):\n",
    "    \"\"\"\n",
    "    Visualize predictions from both methods on a sample image\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to test image\n",
    "        label_path (str): Path to corresponding label file\n",
    "        sam_predictor: SAM predictor instance\n",
    "        pipeline: TwoStagePipeline instance\n",
    "        text_prompt (str): Text prompt to use\n",
    "        conf_threshold (float): YOLO confidence threshold\n",
    "    \"\"\"\n",
    "    # Load image\n",
    "    image = cv2.imread(image_path)\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    img_height, img_width = image.shape[:2]\n",
    "    \n",
    "    # Load ground truth\n",
    "    gt_boxes = []\n",
    "    with open(label_path, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                class_id, x1, y1, x2, y2 = yolo_to_bbox(line, img_width, img_height)\n",
    "                gt_boxes.append((class_id, x1, y1, x2, y2))\n",
    "    \n",
    "    # Method 1: SAM with GT BBox prompts\n",
    "    sam_predictor.set_image(image_rgb)\n",
    "    bbox_masks = []\n",
    "    for class_id, x1, y1, x2, y2 in gt_boxes:\n",
    "        bbox_array = np.array([x1, y1, x2, y2])\n",
    "        mask, _, _ = sam_predictor.predict(box=bbox_array, multimask_output=False)\n",
    "        bbox_masks.append(mask[0])\n",
    "    \n",
    "    bbox_final_mask = np.zeros((img_height, img_width), dtype=bool)\n",
    "    for mask in bbox_masks:\n",
    "        bbox_final_mask = np.logical_or(bbox_final_mask, mask)\n",
    "    \n",
    "    # Method 2: Pipeline with text prompt\n",
    "    text_results = pipeline.process_image(image_path, text_prompt, conf_threshold)\n",
    "    text_final_mask = text_results['final_mask'] if text_results else np.zeros((img_height, img_width), dtype=bool)\n",
    "    \n",
    "    # Ground truth combined mask\n",
    "    gt_combined_mask = np.zeros((img_height, img_width), dtype=bool)\n",
    "    for class_id, x1, y1, x2, y2 in gt_boxes:\n",
    "        gt_mask = create_mask_from_bbox((x1, y1, x2, y2), (img_height, img_width))\n",
    "        gt_combined_mask = np.logical_or(gt_combined_mask, gt_mask)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    bbox_iou = calculate_iou(bbox_final_mask, gt_combined_mask)\n",
    "    bbox_dice = calculate_dice(bbox_final_mask, gt_combined_mask)\n",
    "    text_iou = calculate_iou(text_final_mask, gt_combined_mask)\n",
    "    text_dice = calculate_dice(text_final_mask, gt_combined_mask)\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # Row 1: Method 1 - BBox Prompts\n",
    "    # Original with GT boxes\n",
    "    ax = axes[0, 0]\n",
    "    ax.imshow(image_rgb)\n",
    "    for class_id, x1, y1, x2, y2 in gt_boxes:\n",
    "        rect = plt.Rectangle((x1, y1), x2-x1, y2-y1, fill=False, edgecolor='lime', linewidth=3)\n",
    "        ax.add_patch(rect)\n",
    "    ax.set_title('Ground Truth Boxes', fontsize=12, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # SAM output with bbox prompts\n",
    "    ax = axes[0, 1]\n",
    "    ax.imshow(image_rgb)\n",
    "    colored_mask = np.zeros_like(image_rgb)\n",
    "    colored_mask[bbox_final_mask] = [255, 0, 0]\n",
    "    ax.imshow(colored_mask, alpha=0.5)\n",
    "    ax.set_title('Method 1: SAM + BBox Prompts', fontsize=12, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Metrics for method 1\n",
    "    ax = axes[0, 2]\n",
    "    ax.text(0.5, 0.6, f'IoU: {bbox_iou:.4f}', ha='center', va='center', fontsize=16, fontweight='bold')\n",
    "    ax.text(0.5, 0.4, f'Dice: {bbox_dice:.4f}', ha='center', va='center', fontsize=16, fontweight='bold')\n",
    "    ax.set_title('Method 1 Metrics', fontsize=12, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Row 2: Method 2 - Text Prompts\n",
    "    # Original with YOLO detections\n",
    "    ax = axes[1, 0]\n",
    "    ax.imshow(image_rgb)\n",
    "    if text_results and text_results['boxes']:\n",
    "        for box in text_results['boxes']:\n",
    "            x1, y1, x2, y2 = box\n",
    "            rect = plt.Rectangle((x1, y1), x2-x1, y2-y1, fill=False, edgecolor='yellow', linewidth=3)\n",
    "            ax.add_patch(rect)\n",
    "    ax.set_title(f'YOLO Detections\\n(prompt: \"{text_prompt}\")', fontsize=12, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # SAM output with text prompts\n",
    "    ax = axes[1, 1]\n",
    "    ax.imshow(image_rgb)\n",
    "    colored_mask = np.zeros_like(image_rgb)\n",
    "    colored_mask[text_final_mask] = [0, 255, 0]\n",
    "    ax.imshow(colored_mask, alpha=0.5)\n",
    "    ax.set_title('Method 2: Pipeline + Text Prompts', fontsize=12, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Metrics for method 2\n",
    "    ax = axes[1, 2]\n",
    "    ax.text(0.5, 0.6, f'IoU: {text_iou:.4f}', ha='center', va='center', fontsize=16, fontweight='bold')\n",
    "    ax.text(0.5, 0.4, f'Dice: {text_dice:.4f}', ha='center', va='center', fontsize=16, fontweight='bold')\n",
    "    ax.set_title('Method 2 Metrics', fontsize=12, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Sample: {os.path.basename(image_path)}\")\n",
    "    print(f\"Method 1 (BBox):  IoU={bbox_iou:.4f}, Dice={bbox_dice:.4f}\")\n",
    "    print(f\"Method 2 (Text):  IoU={text_iou:.4f}, Dice={text_dice:.4f}\")\n",
    "    print(f\"Difference:       IoU={bbox_iou-text_iou:+.4f}, Dice={bbox_dice-text_dice:+.4f}\")\n",
    "\n",
    "# Example: Visualize a sample image (update paths as needed)\n",
    "\"\"\"\n",
    "sample_image = 'merged_dataset/test/images/sample.jpg'\n",
    "sample_label = 'merged_dataset/test/labels/sample.txt'\n",
    "\n",
    "visualize_sample_predictions(\n",
    "    image_path=sample_image,\n",
    "    label_path=sample_label,\n",
    "    sam_predictor=sam_predictor,\n",
    "    pipeline=pipeline,\n",
    "    text_prompt='find cracks',  # or 'segment taping area'\n",
    "    conf_threshold=0.3\n",
    ")\n",
    "\"\"\"\n",
    "print(\"Visual comparison function ready! Uncomment and update paths to use.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a67e688",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 📋 Evaluation Summary & Guide\n",
    "\n",
    "### What We're Evaluating\n",
    "\n",
    "We measure segmentation quality using two metrics:\n",
    "\n",
    "1. **mIoU (mean Intersection over Union)**\n",
    "   - Formula: `Intersection / Union`\n",
    "   - Range: 0 to 1 (higher is better)\n",
    "   - Measures: How much predicted and ground truth masks overlap\n",
    "   - Interpretation:\n",
    "     - > 0.9: Excellent\n",
    "     - 0.7-0.9: Good\n",
    "     - 0.5-0.7: Moderate\n",
    "     - < 0.5: Poor\n",
    "\n",
    "2. **Dice Score (F1 Score)**\n",
    "   - Formula: `2 × Intersection / (Predicted + Ground Truth)`\n",
    "   - Range: 0 to 1 (higher is better)\n",
    "   - Measures: Harmonic mean of precision and recall\n",
    "   - More sensitive to false positives/negatives than IoU\n",
    "\n",
    "### Two Evaluation Methods\n",
    "\n",
    "#### Method 1: SAM with Bounding Box Prompts ✅\n",
    "**Purpose**: Measure SAM's segmentation quality with perfect inputs\n",
    "\n",
    "**Process**:\n",
    "1. Load ground truth bounding boxes from YOLO labels\n",
    "2. Feed boxes directly to SAM\n",
    "3. Compare SAM masks with ground truth\n",
    "4. Calculate mIoU and Dice\n",
    "\n",
    "**This tells us**: Upper bound performance - how good SAM is when given perfect bounding boxes\n",
    "\n",
    "---\n",
    "\n",
    "#### Method 2: Complete Pipeline with Text Prompts 🔄\n",
    "**Purpose**: Measure real-world end-to-end performance\n",
    "\n",
    "**Process**:\n",
    "1. User provides text prompt (e.g., \"find cracks\")\n",
    "2. Prompt mapper converts to class\n",
    "3. YOLO detects and filters objects\n",
    "4. SAM segments using YOLO's boxes\n",
    "5. Compare final masks with ground truth\n",
    "6. Calculate mIoU and Dice\n",
    "\n",
    "**This tells us**: Real-world performance including YOLO detection errors\n",
    "\n",
    "---\n",
    "\n",
    "### How to Run the Evaluation\n",
    "\n",
    "**Prerequisites** (must be run first):\n",
    "- Complete Part A (Training) to get trained YOLO model\n",
    "- Complete Part B steps 1-7 to initialize SAM and pipeline\n",
    "\n",
    "**Then run**:\n",
    "1. **Step 11**: Load metric calculation functions\n",
    "2. **Step 12**: Run Method 1 evaluation (BBox prompts)\n",
    "3. **Step 13**: Run Method 2 evaluation (Text prompts)\n",
    "4. **Step 14**: Compare both methods\n",
    "5. **Step 15**: Visualize sample predictions (optional)\n",
    "\n",
    "**Adjust these parameters**:\n",
    "- `max_samples`: Number of test images (None = all, 50 = faster testing)\n",
    "- `conf_threshold`: YOLO confidence (0.3 is balanced, higher = fewer detections)\n",
    "- `test_prompts`: Add/modify text prompts to test\n",
    "\n",
    "---\n",
    "\n",
    "### Interpreting Results\n",
    "\n",
    "**Performance Gap Analysis**:\n",
    "- Gap = Method 1 score - Method 2 score\n",
    "- **Small gap** (< 5%): YOLO detection is excellent\n",
    "- **Medium gap** (5-15%): Some detection errors affecting segmentation\n",
    "- **Large gap** (> 15%): YOLO needs improvement or lower confidence threshold\n",
    "\n",
    "**Per-Class Analysis**:\n",
    "- Compare crack vs drywall-join performance\n",
    "- Identifies which class is harder to detect/segment\n",
    "- May need class-specific threshold tuning\n",
    "\n",
    "---\n",
    "\n",
    "### Expected Results\n",
    "\n",
    "**Typical Performance Ranges**:\n",
    "- **Method 1 (BBox)**: IoU 0.85-0.95, Dice 0.90-0.97\n",
    "  - SAM is very good at segmentation with good boxes\n",
    "  \n",
    "- **Method 2 (Text)**: IoU 0.70-0.85, Dice 0.80-0.92\n",
    "  - Depends heavily on YOLO detection quality\n",
    "  - Text prompt quality also matters\n",
    "\n",
    "**If your scores are low**:\n",
    "1. Check if YOLO model trained properly\n",
    "2. Verify SAM model loaded correctly\n",
    "3. Try adjusting confidence threshold\n",
    "4. Ensure test data has good annotations\n",
    "5. Check if prompt mapping works correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730bd6f6",
   "metadata": {},
   "source": [
    "### Quick Start: Complete Workflow\n",
    "\n",
    "```python\n",
    "# ============================================================\n",
    "# COMPLETE EXECUTION ORDER\n",
    "# ============================================================\n",
    "\n",
    "# --- PART A: ONE-TIME TRAINING (Run these in order) ---\n",
    "# Step 1: Install dependencies\n",
    "# Step 2: Merge datasets\n",
    "# Step 3: Train YOLO model (takes time!)\n",
    "# Step 4: Validate model\n",
    "\n",
    "# --- PART B: INFERENCE SETUP (Run these in order) ---\n",
    "# Step 5: Load SAM model\n",
    "# Step 6: Initialize prompt mapper\n",
    "# Step 7: Create pipeline\n",
    "\n",
    "# --- EVALUATION (Run these in order) ---\n",
    "# Step 11: Load metric functions\n",
    "# Step 12: Evaluate with BBox prompts\n",
    "# Step 13: Evaluate with Text prompts\n",
    "# Step 14: Compare results\n",
    "# Step 15: (Optional) Visualize samples\n",
    "\n",
    "# ============================================================\n",
    "# TYPICAL RESULTS YOU SHOULD SEE\n",
    "# ============================================================\n",
    "\"\"\"\n",
    "Method 1 (BBox Prompts):\n",
    "  Mean IoU:  0.87-0.93\n",
    "  Mean Dice: 0.91-0.95\n",
    "  \n",
    "Method 2 (Text Prompts):\n",
    "  Mean IoU:  0.75-0.88\n",
    "  Mean Dice: 0.82-0.92\n",
    "  \n",
    "Performance Gap: 5-15% (due to YOLO detection errors)\n",
    "\"\"\"\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
